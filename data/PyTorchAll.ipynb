{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_basics.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 0.0\n",
      "\t 1.0 2.0 0.0 4.0\n",
      "\t 2.0 4.0 0.0 16.0\n",
      "\t 3.0 6.0 0.0 36.0\n",
      "MSE= 18.666666666666668\n",
      "w= 0.1\n",
      "\t 1.0 2.0 0.1 3.61\n",
      "\t 2.0 4.0 0.2 14.44\n",
      "\t 3.0 6.0 0.30000000000000004 32.49\n",
      "MSE= 16.846666666666668\n",
      "w= 0.2\n",
      "\t 1.0 2.0 0.2 3.24\n",
      "\t 2.0 4.0 0.4 12.96\n",
      "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
      "MSE= 15.120000000000003\n",
      "w= 0.30000000000000004\n",
      "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
      "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
      "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
      "MSE= 13.486666666666665\n",
      "w= 0.4\n",
      "\t 1.0 2.0 0.4 2.5600000000000005\n",
      "\t 2.0 4.0 0.8 10.240000000000002\n",
      "\t 3.0 6.0 1.2000000000000002 23.04\n",
      "MSE= 11.946666666666667\n",
      "w= 0.5\n",
      "\t 1.0 2.0 0.5 2.25\n",
      "\t 2.0 4.0 1.0 9.0\n",
      "\t 3.0 6.0 1.5 20.25\n",
      "MSE= 10.5\n",
      "w= 0.6000000000000001\n",
      "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
      "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
      "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
      "MSE= 9.146666666666663\n",
      "w= 0.7000000000000001\n",
      "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
      "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
      "\t 3.0 6.0 2.1 15.209999999999999\n",
      "MSE= 7.886666666666666\n",
      "w= 0.8\n",
      "\t 1.0 2.0 0.8 1.44\n",
      "\t 2.0 4.0 1.6 5.76\n",
      "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
      "MSE= 6.719999999999999\n",
      "w= 0.9\n",
      "\t 1.0 2.0 0.9 1.2100000000000002\n",
      "\t 2.0 4.0 1.8 4.840000000000001\n",
      "\t 3.0 6.0 2.7 10.889999999999999\n",
      "MSE= 5.646666666666666\n",
      "w= 1.0\n",
      "\t 1.0 2.0 1.0 1.0\n",
      "\t 2.0 4.0 2.0 4.0\n",
      "\t 3.0 6.0 3.0 9.0\n",
      "MSE= 4.666666666666667\n",
      "w= 1.1\n",
      "\t 1.0 2.0 1.1 0.8099999999999998\n",
      "\t 2.0 4.0 2.2 3.2399999999999993\n",
      "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
      "MSE= 3.779999999999999\n",
      "w= 1.2000000000000002\n",
      "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
      "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
      "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
      "MSE= 2.986666666666665\n",
      "w= 1.3\n",
      "\t 1.0 2.0 1.3 0.48999999999999994\n",
      "\t 2.0 4.0 2.6 1.9599999999999997\n",
      "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
      "MSE= 2.2866666666666657\n",
      "w= 1.4000000000000001\n",
      "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
      "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
      "\t 3.0 6.0 4.2 3.2399999999999993\n",
      "MSE= 1.6799999999999995\n",
      "w= 1.5\n",
      "\t 1.0 2.0 1.5 0.25\n",
      "\t 2.0 4.0 3.0 1.0\n",
      "\t 3.0 6.0 4.5 2.25\n",
      "MSE= 1.1666666666666667\n",
      "w= 1.6\n",
      "\t 1.0 2.0 1.6 0.15999999999999992\n",
      "\t 2.0 4.0 3.2 0.6399999999999997\n",
      "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
      "MSE= 0.746666666666666\n",
      "w= 1.7000000000000002\n",
      "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
      "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
      "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
      "MSE= 0.4199999999999995\n",
      "w= 1.8\n",
      "\t 1.0 2.0 1.8 0.03999999999999998\n",
      "\t 2.0 4.0 3.6 0.15999999999999992\n",
      "\t 3.0 6.0 5.4 0.3599999999999996\n",
      "MSE= 0.1866666666666665\n",
      "w= 1.9000000000000001\n",
      "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
      "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
      "\t 3.0 6.0 5.7 0.0899999999999999\n",
      "MSE= 0.046666666666666586\n",
      "w= 2.0\n",
      "\t 1.0 2.0 2.0 0.0\n",
      "\t 2.0 4.0 4.0 0.0\n",
      "\t 3.0 6.0 6.0 0.0\n",
      "MSE= 0.0\n",
      "w= 2.1\n",
      "\t 1.0 2.0 2.1 0.010000000000000018\n",
      "\t 2.0 4.0 4.2 0.04000000000000007\n",
      "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
      "MSE= 0.046666666666666835\n",
      "w= 2.2\n",
      "\t 1.0 2.0 2.2 0.04000000000000007\n",
      "\t 2.0 4.0 4.4 0.16000000000000028\n",
      "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
      "MSE= 0.18666666666666698\n",
      "w= 2.3000000000000003\n",
      "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
      "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
      "\t 3.0 6.0 6.9 0.8100000000000006\n",
      "MSE= 0.42000000000000054\n",
      "w= 2.4000000000000004\n",
      "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
      "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
      "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
      "MSE= 0.7466666666666679\n",
      "w= 2.5\n",
      "\t 1.0 2.0 2.5 0.25\n",
      "\t 2.0 4.0 5.0 1.0\n",
      "\t 3.0 6.0 7.5 2.25\n",
      "MSE= 1.1666666666666667\n",
      "w= 2.6\n",
      "\t 1.0 2.0 2.6 0.3600000000000001\n",
      "\t 2.0 4.0 5.2 1.4400000000000004\n",
      "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
      "MSE= 1.6800000000000008\n",
      "w= 2.7\n",
      "\t 1.0 2.0 2.7 0.49000000000000027\n",
      "\t 2.0 4.0 5.4 1.960000000000001\n",
      "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
      "MSE= 2.2866666666666693\n",
      "w= 2.8000000000000003\n",
      "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
      "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
      "\t 3.0 6.0 8.4 5.760000000000002\n",
      "MSE= 2.986666666666668\n",
      "w= 2.9000000000000004\n",
      "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
      "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
      "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
      "MSE= 3.780000000000003\n",
      "w= 3.0\n",
      "\t 1.0 2.0 3.0 1.0\n",
      "\t 2.0 4.0 6.0 4.0\n",
      "\t 3.0 6.0 9.0 9.0\n",
      "MSE= 4.666666666666667\n",
      "w= 3.1\n",
      "\t 1.0 2.0 3.1 1.2100000000000002\n",
      "\t 2.0 4.0 6.2 4.840000000000001\n",
      "\t 3.0 6.0 9.3 10.890000000000004\n",
      "MSE= 5.646666666666668\n",
      "w= 3.2\n",
      "\t 1.0 2.0 3.2 1.4400000000000004\n",
      "\t 2.0 4.0 6.4 5.760000000000002\n",
      "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
      "MSE= 6.720000000000003\n",
      "w= 3.3000000000000003\n",
      "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
      "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
      "\t 3.0 6.0 9.9 15.210000000000003\n",
      "MSE= 7.886666666666668\n",
      "w= 3.4000000000000004\n",
      "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
      "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
      "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
      "MSE= 9.14666666666667\n",
      "w= 3.5\n",
      "\t 1.0 2.0 3.5 2.25\n",
      "\t 2.0 4.0 7.0 9.0\n",
      "\t 3.0 6.0 10.5 20.25\n",
      "MSE= 10.5\n",
      "w= 3.6\n",
      "\t 1.0 2.0 3.6 2.5600000000000005\n",
      "\t 2.0 4.0 7.2 10.240000000000002\n",
      "\t 3.0 6.0 10.8 23.040000000000006\n",
      "MSE= 11.94666666666667\n",
      "w= 3.7\n",
      "\t 1.0 2.0 3.7 2.8900000000000006\n",
      "\t 2.0 4.0 7.4 11.560000000000002\n",
      "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
      "MSE= 13.486666666666673\n",
      "w= 3.8000000000000003\n",
      "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
      "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
      "\t 3.0 6.0 11.4 29.160000000000004\n",
      "MSE= 15.120000000000005\n",
      "w= 3.9000000000000004\n",
      "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
      "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
      "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
      "MSE= 16.84666666666667\n",
      "w= 4.0\n",
      "\t 1.0 2.0 4.0 4.0\n",
      "\t 2.0 4.0 8.0 16.0\n",
      "\t 3.0 6.0 12.0 36.0\n",
      "MSE= 18.666666666666668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "\n",
    "# our model for the forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "w_list = []\n",
    "mse_list = []\n",
    "\n",
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "    print(\"w=\", w)\n",
    "    l_sum = 0\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
    "    print(\"MSE=\", l_sum / 3)\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum / 3)\n",
    "\n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_Manual Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.23\n",
      "progress: 0 w= 1.26 loss= 4.92\n",
      "\tgrad:  1.0 2.0 -1.48\n",
      "\tgrad:  2.0 4.0 -5.8\n",
      "\tgrad:  3.0 6.0 -12.0\n",
      "progress: 1 w= 1.45 loss= 2.69\n",
      "\tgrad:  1.0 2.0 -1.09\n",
      "\tgrad:  2.0 4.0 -4.29\n",
      "\tgrad:  3.0 6.0 -8.87\n",
      "progress: 2 w= 1.6 loss= 1.47\n",
      "\tgrad:  1.0 2.0 -0.81\n",
      "\tgrad:  2.0 4.0 -3.17\n",
      "\tgrad:  3.0 6.0 -6.56\n",
      "progress: 3 w= 1.7 loss= 0.8\n",
      "\tgrad:  1.0 2.0 -0.6\n",
      "\tgrad:  2.0 4.0 -2.34\n",
      "\tgrad:  3.0 6.0 -4.85\n",
      "progress: 4 w= 1.78 loss= 0.44\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.73\n",
      "\tgrad:  3.0 6.0 -3.58\n",
      "progress: 5 w= 1.84 loss= 0.24\n",
      "\tgrad:  1.0 2.0 -0.33\n",
      "\tgrad:  2.0 4.0 -1.28\n",
      "\tgrad:  3.0 6.0 -2.65\n",
      "progress: 6 w= 1.88 loss= 0.13\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.95\n",
      "\tgrad:  3.0 6.0 -1.96\n",
      "progress: 7 w= 1.91 loss= 0.07\n",
      "\tgrad:  1.0 2.0 -0.18\n",
      "\tgrad:  2.0 4.0 -0.7\n",
      "\tgrad:  3.0 6.0 -1.45\n",
      "progress: 8 w= 1.93 loss= 0.04\n",
      "\tgrad:  1.0 2.0 -0.13\n",
      "\tgrad:  2.0 4.0 -0.52\n",
      "\tgrad:  3.0 6.0 -1.07\n",
      "progress: 9 w= 1.95 loss= 0.02\n",
      "predict (after training) 4 hours 7.804863933862125\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0  # a random guess: random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# compute gradient\n",
    "def gradient(x, y):  # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "       # print(\"weight\", w)\n",
    "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
    "        l = loss(x_val, y_val)\n",
    "\n",
    "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\",  \"4 hours\", forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Auto-Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor(4.)\n",
      "\tgrad:  1.0 2.0 tensor(-2.)\n",
      "\tgrad:  2.0 4.0 tensor(-7.8400)\n",
      "\tgrad:  3.0 6.0 tensor(-16.2288)\n",
      "progress: 0 tensor(7.3159)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4786)\n",
      "\tgrad:  2.0 4.0 tensor(-5.7962)\n",
      "\tgrad:  3.0 6.0 tensor(-11.9981)\n",
      "progress: 1 tensor(3.9988)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0932)\n",
      "\tgrad:  2.0 4.0 tensor(-4.2852)\n",
      "\tgrad:  3.0 6.0 tensor(-8.8704)\n",
      "progress: 2 tensor(2.1857)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8082)\n",
      "\tgrad:  2.0 4.0 tensor(-3.1681)\n",
      "\tgrad:  3.0 6.0 tensor(-6.5580)\n",
      "progress: 3 tensor(1.1946)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5975)\n",
      "\tgrad:  2.0 4.0 tensor(-2.3422)\n",
      "\tgrad:  3.0 6.0 tensor(-4.8484)\n",
      "progress: 4 tensor(0.6530)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4417)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7316)\n",
      "\tgrad:  3.0 6.0 tensor(-3.5845)\n",
      "progress: 5 tensor(0.3569)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3266)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2802)\n",
      "\tgrad:  3.0 6.0 tensor(-2.6500)\n",
      "progress: 6 tensor(0.1951)\n",
      "\tgrad:  1.0 2.0 tensor(-0.2414)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9465)\n",
      "\tgrad:  3.0 6.0 tensor(-1.9592)\n",
      "progress: 7 tensor(0.1066)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1785)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6997)\n",
      "\tgrad:  3.0 6.0 tensor(-1.4485)\n",
      "progress: 8 tensor(0.0583)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1320)\n",
      "\tgrad:  2.0 4.0 tensor(-0.5173)\n",
      "\tgrad:  3.0 6.0 tensor(-1.0709)\n",
      "progress: 9 tensor(0.0319)\n",
      "predict (after training) 4 tensor(7.8049)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\",  4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(36.9267)\n",
      "1 tensor(16.5235)\n",
      "2 tensor(7.4394)\n",
      "3 tensor(3.3942)\n",
      "4 tensor(1.5922)\n",
      "5 tensor(0.7889)\n",
      "6 tensor(0.4301)\n",
      "7 tensor(0.2692)\n",
      "8 tensor(0.1965)\n",
      "9 tensor(0.1630)\n",
      "10 tensor(0.1470)\n",
      "11 tensor(0.1388)\n",
      "12 tensor(0.1341)\n",
      "13 tensor(0.1310)\n",
      "14 tensor(0.1286)\n",
      "15 tensor(0.1265)\n",
      "16 tensor(0.1246)\n",
      "17 tensor(0.1227)\n",
      "18 tensor(0.1210)\n",
      "19 tensor(0.1192)\n",
      "20 tensor(0.1175)\n",
      "21 tensor(0.1158)\n",
      "22 tensor(0.1141)\n",
      "23 tensor(0.1125)\n",
      "24 tensor(0.1109)\n",
      "25 tensor(0.1093)\n",
      "26 tensor(0.1077)\n",
      "27 tensor(0.1062)\n",
      "28 tensor(0.1046)\n",
      "29 tensor(0.1031)\n",
      "30 tensor(0.1017)\n",
      "31 tensor(0.1002)\n",
      "32 tensor(0.0988)\n",
      "33 tensor(0.0973)\n",
      "34 tensor(0.0959)\n",
      "35 tensor(0.0946)\n",
      "36 tensor(0.0932)\n",
      "37 tensor(0.0919)\n",
      "38 tensor(0.0905)\n",
      "39 tensor(0.0892)\n",
      "40 tensor(0.0880)\n",
      "41 tensor(0.0867)\n",
      "42 tensor(0.0854)\n",
      "43 tensor(0.0842)\n",
      "44 tensor(0.0830)\n",
      "45 tensor(0.0818)\n",
      "46 tensor(0.0806)\n",
      "47 tensor(0.0795)\n",
      "48 tensor(0.0783)\n",
      "49 tensor(0.0772)\n",
      "50 tensor(0.0761)\n",
      "51 tensor(0.0750)\n",
      "52 tensor(0.0739)\n",
      "53 tensor(0.0729)\n",
      "54 tensor(0.0718)\n",
      "55 tensor(0.0708)\n",
      "56 tensor(0.0698)\n",
      "57 tensor(0.0688)\n",
      "58 tensor(0.0678)\n",
      "59 tensor(0.0668)\n",
      "60 tensor(0.0658)\n",
      "61 tensor(0.0649)\n",
      "62 tensor(0.0640)\n",
      "63 tensor(0.0630)\n",
      "64 tensor(0.0621)\n",
      "65 tensor(0.0612)\n",
      "66 tensor(0.0604)\n",
      "67 tensor(0.0595)\n",
      "68 tensor(0.0586)\n",
      "69 tensor(0.0578)\n",
      "70 tensor(0.0570)\n",
      "71 tensor(0.0562)\n",
      "72 tensor(0.0553)\n",
      "73 tensor(0.0545)\n",
      "74 tensor(0.0538)\n",
      "75 tensor(0.0530)\n",
      "76 tensor(0.0522)\n",
      "77 tensor(0.0515)\n",
      "78 tensor(0.0507)\n",
      "79 tensor(0.0500)\n",
      "80 tensor(0.0493)\n",
      "81 tensor(0.0486)\n",
      "82 tensor(0.0479)\n",
      "83 tensor(0.0472)\n",
      "84 tensor(0.0465)\n",
      "85 tensor(0.0459)\n",
      "86 tensor(0.0452)\n",
      "87 tensor(0.0445)\n",
      "88 tensor(0.0439)\n",
      "89 tensor(0.0433)\n",
      "90 tensor(0.0426)\n",
      "91 tensor(0.0420)\n",
      "92 tensor(0.0414)\n",
      "93 tensor(0.0408)\n",
      "94 tensor(0.0403)\n",
      "95 tensor(0.0397)\n",
      "96 tensor(0.0391)\n",
      "97 tensor(0.0385)\n",
      "98 tensor(0.0380)\n",
      "99 tensor(0.0374)\n",
      "100 tensor(0.0369)\n",
      "101 tensor(0.0364)\n",
      "102 tensor(0.0358)\n",
      "103 tensor(0.0353)\n",
      "104 tensor(0.0348)\n",
      "105 tensor(0.0343)\n",
      "106 tensor(0.0338)\n",
      "107 tensor(0.0333)\n",
      "108 tensor(0.0329)\n",
      "109 tensor(0.0324)\n",
      "110 tensor(0.0319)\n",
      "111 tensor(0.0315)\n",
      "112 tensor(0.0310)\n",
      "113 tensor(0.0306)\n",
      "114 tensor(0.0301)\n",
      "115 tensor(0.0297)\n",
      "116 tensor(0.0293)\n",
      "117 tensor(0.0289)\n",
      "118 tensor(0.0284)\n",
      "119 tensor(0.0280)\n",
      "120 tensor(0.0276)\n",
      "121 tensor(0.0272)\n",
      "122 tensor(0.0268)\n",
      "123 tensor(0.0265)\n",
      "124 tensor(0.0261)\n",
      "125 tensor(0.0257)\n",
      "126 tensor(0.0253)\n",
      "127 tensor(0.0250)\n",
      "128 tensor(0.0246)\n",
      "129 tensor(0.0243)\n",
      "130 tensor(0.0239)\n",
      "131 tensor(0.0236)\n",
      "132 tensor(0.0232)\n",
      "133 tensor(0.0229)\n",
      "134 tensor(0.0226)\n",
      "135 tensor(0.0222)\n",
      "136 tensor(0.0219)\n",
      "137 tensor(0.0216)\n",
      "138 tensor(0.0213)\n",
      "139 tensor(0.0210)\n",
      "140 tensor(0.0207)\n",
      "141 tensor(0.0204)\n",
      "142 tensor(0.0201)\n",
      "143 tensor(0.0198)\n",
      "144 tensor(0.0195)\n",
      "145 tensor(0.0192)\n",
      "146 tensor(0.0190)\n",
      "147 tensor(0.0187)\n",
      "148 tensor(0.0184)\n",
      "149 tensor(0.0182)\n",
      "150 tensor(0.0179)\n",
      "151 tensor(0.0176)\n",
      "152 tensor(0.0174)\n",
      "153 tensor(0.0171)\n",
      "154 tensor(0.0169)\n",
      "155 tensor(0.0166)\n",
      "156 tensor(0.0164)\n",
      "157 tensor(0.0162)\n",
      "158 tensor(0.0159)\n",
      "159 tensor(0.0157)\n",
      "160 tensor(0.0155)\n",
      "161 tensor(0.0153)\n",
      "162 tensor(0.0150)\n",
      "163 tensor(0.0148)\n",
      "164 tensor(0.0146)\n",
      "165 tensor(0.0144)\n",
      "166 tensor(0.0142)\n",
      "167 tensor(0.0140)\n",
      "168 tensor(0.0138)\n",
      "169 tensor(0.0136)\n",
      "170 tensor(0.0134)\n",
      "171 tensor(0.0132)\n",
      "172 tensor(0.0130)\n",
      "173 tensor(0.0128)\n",
      "174 tensor(0.0126)\n",
      "175 tensor(0.0125)\n",
      "176 tensor(0.0123)\n",
      "177 tensor(0.0121)\n",
      "178 tensor(0.0119)\n",
      "179 tensor(0.0118)\n",
      "180 tensor(0.0116)\n",
      "181 tensor(0.0114)\n",
      "182 tensor(0.0113)\n",
      "183 tensor(0.0111)\n",
      "184 tensor(0.0109)\n",
      "185 tensor(0.0108)\n",
      "186 tensor(0.0106)\n",
      "187 tensor(0.0105)\n",
      "188 tensor(0.0103)\n",
      "189 tensor(0.0102)\n",
      "190 tensor(0.0100)\n",
      "191 tensor(0.0099)\n",
      "192 tensor(0.0097)\n",
      "193 tensor(0.0096)\n",
      "194 tensor(0.0095)\n",
      "195 tensor(0.0093)\n",
      "196 tensor(0.0092)\n",
      "197 tensor(0.0091)\n",
      "198 tensor(0.0089)\n",
      "199 tensor(0.0088)\n",
      "200 tensor(0.0087)\n",
      "201 tensor(0.0086)\n",
      "202 tensor(0.0084)\n",
      "203 tensor(0.0083)\n",
      "204 tensor(0.0082)\n",
      "205 tensor(0.0081)\n",
      "206 tensor(0.0080)\n",
      "207 tensor(0.0078)\n",
      "208 tensor(0.0077)\n",
      "209 tensor(0.0076)\n",
      "210 tensor(0.0075)\n",
      "211 tensor(0.0074)\n",
      "212 tensor(0.0073)\n",
      "213 tensor(0.0072)\n",
      "214 tensor(0.0071)\n",
      "215 tensor(0.0070)\n",
      "216 tensor(0.0069)\n",
      "217 tensor(0.0068)\n",
      "218 tensor(0.0067)\n",
      "219 tensor(0.0066)\n",
      "220 tensor(0.0065)\n",
      "221 tensor(0.0064)\n",
      "222 tensor(0.0063)\n",
      "223 tensor(0.0062)\n",
      "224 tensor(0.0061)\n",
      "225 tensor(0.0060)\n",
      "226 tensor(0.0060)\n",
      "227 tensor(0.0059)\n",
      "228 tensor(0.0058)\n",
      "229 tensor(0.0057)\n",
      "230 tensor(0.0056)\n",
      "231 tensor(0.0055)\n",
      "232 tensor(0.0055)\n",
      "233 tensor(0.0054)\n",
      "234 tensor(0.0053)\n",
      "235 tensor(0.0052)\n",
      "236 tensor(0.0052)\n",
      "237 tensor(0.0051)\n",
      "238 tensor(0.0050)\n",
      "239 tensor(0.0049)\n",
      "240 tensor(0.0049)\n",
      "241 tensor(0.0048)\n",
      "242 tensor(0.0047)\n",
      "243 tensor(0.0047)\n",
      "244 tensor(0.0046)\n",
      "245 tensor(0.0045)\n",
      "246 tensor(0.0045)\n",
      "247 tensor(0.0044)\n",
      "248 tensor(0.0043)\n",
      "249 tensor(0.0043)\n",
      "250 tensor(0.0042)\n",
      "251 tensor(0.0041)\n",
      "252 tensor(0.0041)\n",
      "253 tensor(0.0040)\n",
      "254 tensor(0.0040)\n",
      "255 tensor(0.0039)\n",
      "256 tensor(0.0039)\n",
      "257 tensor(0.0038)\n",
      "258 tensor(0.0037)\n",
      "259 tensor(0.0037)\n",
      "260 tensor(0.0036)\n",
      "261 tensor(0.0036)\n",
      "262 tensor(0.0035)\n",
      "263 tensor(0.0035)\n",
      "264 tensor(0.0034)\n",
      "265 tensor(0.0034)\n",
      "266 tensor(0.0033)\n",
      "267 tensor(0.0033)\n",
      "268 tensor(0.0032)\n",
      "269 tensor(0.0032)\n",
      "270 tensor(0.0031)\n",
      "271 tensor(0.0031)\n",
      "272 tensor(0.0031)\n",
      "273 tensor(0.0030)\n",
      "274 tensor(0.0030)\n",
      "275 tensor(0.0029)\n",
      "276 tensor(0.0029)\n",
      "277 tensor(0.0028)\n",
      "278 tensor(0.0028)\n",
      "279 tensor(0.0028)\n",
      "280 tensor(0.0027)\n",
      "281 tensor(0.0027)\n",
      "282 tensor(0.0026)\n",
      "283 tensor(0.0026)\n",
      "284 tensor(0.0026)\n",
      "285 tensor(0.0025)\n",
      "286 tensor(0.0025)\n",
      "287 tensor(0.0025)\n",
      "288 tensor(0.0024)\n",
      "289 tensor(0.0024)\n",
      "290 tensor(0.0024)\n",
      "291 tensor(0.0023)\n",
      "292 tensor(0.0023)\n",
      "293 tensor(0.0023)\n",
      "294 tensor(0.0022)\n",
      "295 tensor(0.0022)\n",
      "296 tensor(0.0022)\n",
      "297 tensor(0.0021)\n",
      "298 tensor(0.0021)\n",
      "299 tensor(0.0021)\n",
      "300 tensor(0.0020)\n",
      "301 tensor(0.0020)\n",
      "302 tensor(0.0020)\n",
      "303 tensor(0.0020)\n",
      "304 tensor(0.0019)\n",
      "305 tensor(0.0019)\n",
      "306 tensor(0.0019)\n",
      "307 tensor(0.0018)\n",
      "308 tensor(0.0018)\n",
      "309 tensor(0.0018)\n",
      "310 tensor(0.0018)\n",
      "311 tensor(0.0017)\n",
      "312 tensor(0.0017)\n",
      "313 tensor(0.0017)\n",
      "314 tensor(0.0017)\n",
      "315 tensor(0.0016)\n",
      "316 tensor(0.0016)\n",
      "317 tensor(0.0016)\n",
      "318 tensor(0.0016)\n",
      "319 tensor(0.0015)\n",
      "320 tensor(0.0015)\n",
      "321 tensor(0.0015)\n",
      "322 tensor(0.0015)\n",
      "323 tensor(0.0015)\n",
      "324 tensor(0.0014)\n",
      "325 tensor(0.0014)\n",
      "326 tensor(0.0014)\n",
      "327 tensor(0.0014)\n",
      "328 tensor(0.0014)\n",
      "329 tensor(0.0013)\n",
      "330 tensor(0.0013)\n",
      "331 tensor(0.0013)\n",
      "332 tensor(0.0013)\n",
      "333 tensor(0.0013)\n",
      "334 tensor(0.0012)\n",
      "335 tensor(0.0012)\n",
      "336 tensor(0.0012)\n",
      "337 tensor(0.0012)\n",
      "338 tensor(0.0012)\n",
      "339 tensor(0.0012)\n",
      "340 tensor(0.0011)\n",
      "341 tensor(0.0011)\n",
      "342 tensor(0.0011)\n",
      "343 tensor(0.0011)\n",
      "344 tensor(0.0011)\n",
      "345 tensor(0.0011)\n",
      "346 tensor(0.0010)\n",
      "347 tensor(0.0010)\n",
      "348 tensor(0.0010)\n",
      "349 tensor(0.0010)\n",
      "350 tensor(0.0010)\n",
      "351 tensor(0.0010)\n",
      "352 tensor(0.0010)\n",
      "353 tensor(0.0009)\n",
      "354 tensor(0.0009)\n",
      "355 tensor(0.0009)\n",
      "356 tensor(0.0009)\n",
      "357 tensor(0.0009)\n",
      "358 tensor(0.0009)\n",
      "359 tensor(0.0009)\n",
      "360 tensor(0.0009)\n",
      "361 tensor(0.0008)\n",
      "362 tensor(0.0008)\n",
      "363 tensor(0.0008)\n",
      "364 tensor(0.0008)\n",
      "365 tensor(0.0008)\n",
      "366 tensor(0.0008)\n",
      "367 tensor(0.0008)\n",
      "368 tensor(0.0008)\n",
      "369 tensor(0.0008)\n",
      "370 tensor(0.0007)\n",
      "371 tensor(0.0007)\n",
      "372 tensor(0.0007)\n",
      "373 tensor(0.0007)\n",
      "374 tensor(0.0007)\n",
      "375 tensor(0.0007)\n",
      "376 tensor(0.0007)\n",
      "377 tensor(0.0007)\n",
      "378 tensor(0.0007)\n",
      "379 tensor(0.0007)\n",
      "380 tensor(0.0006)\n",
      "381 tensor(0.0006)\n",
      "382 tensor(0.0006)\n",
      "383 tensor(0.0006)\n",
      "384 tensor(0.0006)\n",
      "385 tensor(0.0006)\n",
      "386 tensor(0.0006)\n",
      "387 tensor(0.0006)\n",
      "388 tensor(0.0006)\n",
      "389 tensor(0.0006)\n",
      "390 tensor(0.0006)\n",
      "391 tensor(0.0005)\n",
      "392 tensor(0.0005)\n",
      "393 tensor(0.0005)\n",
      "394 tensor(0.0005)\n",
      "395 tensor(0.0005)\n",
      "396 tensor(0.0005)\n",
      "397 tensor(0.0005)\n",
      "398 tensor(0.0005)\n",
      "399 tensor(0.0005)\n",
      "400 tensor(0.0005)\n",
      "401 tensor(0.0005)\n",
      "402 tensor(0.0005)\n",
      "403 tensor(0.0005)\n",
      "404 tensor(0.0005)\n",
      "405 tensor(0.0004)\n",
      "406 tensor(0.0004)\n",
      "407 tensor(0.0004)\n",
      "408 tensor(0.0004)\n",
      "409 tensor(0.0004)\n",
      "410 tensor(0.0004)\n",
      "411 tensor(0.0004)\n",
      "412 tensor(0.0004)\n",
      "413 tensor(0.0004)\n",
      "414 tensor(0.0004)\n",
      "415 tensor(0.0004)\n",
      "416 tensor(0.0004)\n",
      "417 tensor(0.0004)\n",
      "418 tensor(0.0004)\n",
      "419 tensor(0.0004)\n",
      "420 tensor(0.0004)\n",
      "421 tensor(0.0004)\n",
      "422 tensor(0.0003)\n",
      "423 tensor(0.0003)\n",
      "424 tensor(0.0003)\n",
      "425 tensor(0.0003)\n",
      "426 tensor(0.0003)\n",
      "427 tensor(0.0003)\n",
      "428 tensor(0.0003)\n",
      "429 tensor(0.0003)\n",
      "430 tensor(0.0003)\n",
      "431 tensor(0.0003)\n",
      "432 tensor(0.0003)\n",
      "433 tensor(0.0003)\n",
      "434 tensor(0.0003)\n",
      "435 tensor(0.0003)\n",
      "436 tensor(0.0003)\n",
      "437 tensor(0.0003)\n",
      "438 tensor(0.0003)\n",
      "439 tensor(0.0003)\n",
      "440 tensor(0.0003)\n",
      "441 tensor(0.0003)\n",
      "442 tensor(0.0003)\n",
      "443 tensor(0.0003)\n",
      "444 tensor(0.0003)\n",
      "445 tensor(0.0003)\n",
      "446 tensor(0.0002)\n",
      "447 tensor(0.0002)\n",
      "448 tensor(0.0002)\n",
      "449 tensor(0.0002)\n",
      "450 tensor(0.0002)\n",
      "451 tensor(0.0002)\n",
      "452 tensor(0.0002)\n",
      "453 tensor(0.0002)\n",
      "454 tensor(0.0002)\n",
      "455 tensor(0.0002)\n",
      "456 tensor(0.0002)\n",
      "457 tensor(0.0002)\n",
      "458 tensor(0.0002)\n",
      "459 tensor(0.0002)\n",
      "460 tensor(0.0002)\n",
      "461 tensor(0.0002)\n",
      "462 tensor(0.0002)\n",
      "463 tensor(0.0002)\n",
      "464 tensor(0.0002)\n",
      "465 tensor(0.0002)\n",
      "466 tensor(0.0002)\n",
      "467 tensor(0.0002)\n",
      "468 tensor(0.0002)\n",
      "469 tensor(0.0002)\n",
      "470 tensor(0.0002)\n",
      "471 tensor(0.0002)\n",
      "472 tensor(0.0002)\n",
      "473 tensor(0.0002)\n",
      "474 tensor(0.0002)\n",
      "475 tensor(0.0002)\n",
      "476 tensor(0.0002)\n",
      "477 tensor(0.0002)\n",
      "478 tensor(0.0002)\n",
      "479 tensor(0.0002)\n",
      "480 tensor(0.0002)\n",
      "481 tensor(0.0001)\n",
      "482 tensor(0.0001)\n",
      "483 tensor(0.0001)\n",
      "484 tensor(0.0001)\n",
      "485 tensor(0.0001)\n",
      "486 tensor(0.0001)\n",
      "487 tensor(0.0001)\n",
      "488 tensor(0.0001)\n",
      "489 tensor(0.0001)\n",
      "490 tensor(0.0001)\n",
      "491 tensor(0.0001)\n",
      "492 tensor(0.0001)\n",
      "493 tensor(0.0001)\n",
      "494 tensor(0.0001)\n",
      "495 tensor(0.0001)\n",
      "496 tensor(0.0001)\n",
      "497 tensor(0.0001)\n",
      "498 tensor(0.0001)\n",
      "499 tensor(0.0001)\n",
      "predict (after training) 4 tensor(7.9877)\n"
     ]
    }
   ],
   "source": [
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)\",  4, model(hour_var).data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_Rogistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.7836)\n",
      "1 tensor(0.7830)\n",
      "2 tensor(0.7824)\n",
      "3 tensor(0.7818)\n",
      "4 tensor(0.7812)\n",
      "5 tensor(0.7807)\n",
      "6 tensor(0.7801)\n",
      "7 tensor(0.7795)\n",
      "8 tensor(0.7790)\n",
      "9 tensor(0.7784)\n",
      "10 tensor(0.7779)\n",
      "11 tensor(0.7774)\n",
      "12 tensor(0.7769)\n",
      "13 tensor(0.7763)\n",
      "14 tensor(0.7758)\n",
      "15 tensor(0.7753)\n",
      "16 tensor(0.7748)\n",
      "17 tensor(0.7743)\n",
      "18 tensor(0.7739)\n",
      "19 tensor(0.7734)\n",
      "20 tensor(0.7729)\n",
      "21 tensor(0.7724)\n",
      "22 tensor(0.7720)\n",
      "23 tensor(0.7715)\n",
      "24 tensor(0.7711)\n",
      "25 tensor(0.7706)\n",
      "26 tensor(0.7702)\n",
      "27 tensor(0.7697)\n",
      "28 tensor(0.7693)\n",
      "29 tensor(0.7688)\n",
      "30 tensor(0.7684)\n",
      "31 tensor(0.7680)\n",
      "32 tensor(0.7676)\n",
      "33 tensor(0.7671)\n",
      "34 tensor(0.7667)\n",
      "35 tensor(0.7663)\n",
      "36 tensor(0.7659)\n",
      "37 tensor(0.7655)\n",
      "38 tensor(0.7651)\n",
      "39 tensor(0.7647)\n",
      "40 tensor(0.7643)\n",
      "41 tensor(0.7639)\n",
      "42 tensor(0.7635)\n",
      "43 tensor(0.7631)\n",
      "44 tensor(0.7627)\n",
      "45 tensor(0.7623)\n",
      "46 tensor(0.7619)\n",
      "47 tensor(0.7615)\n",
      "48 tensor(0.7611)\n",
      "49 tensor(0.7607)\n",
      "50 tensor(0.7603)\n",
      "51 tensor(0.7600)\n",
      "52 tensor(0.7596)\n",
      "53 tensor(0.7592)\n",
      "54 tensor(0.7588)\n",
      "55 tensor(0.7584)\n",
      "56 tensor(0.7581)\n",
      "57 tensor(0.7577)\n",
      "58 tensor(0.7573)\n",
      "59 tensor(0.7570)\n",
      "60 tensor(0.7566)\n",
      "61 tensor(0.7562)\n",
      "62 tensor(0.7558)\n",
      "63 tensor(0.7555)\n",
      "64 tensor(0.7551)\n",
      "65 tensor(0.7548)\n",
      "66 tensor(0.7544)\n",
      "67 tensor(0.7540)\n",
      "68 tensor(0.7537)\n",
      "69 tensor(0.7533)\n",
      "70 tensor(0.7530)\n",
      "71 tensor(0.7526)\n",
      "72 tensor(0.7522)\n",
      "73 tensor(0.7519)\n",
      "74 tensor(0.7515)\n",
      "75 tensor(0.7512)\n",
      "76 tensor(0.7508)\n",
      "77 tensor(0.7505)\n",
      "78 tensor(0.7501)\n",
      "79 tensor(0.7498)\n",
      "80 tensor(0.7494)\n",
      "81 tensor(0.7491)\n",
      "82 tensor(0.7487)\n",
      "83 tensor(0.7484)\n",
      "84 tensor(0.7480)\n",
      "85 tensor(0.7477)\n",
      "86 tensor(0.7473)\n",
      "87 tensor(0.7470)\n",
      "88 tensor(0.7466)\n",
      "89 tensor(0.7463)\n",
      "90 tensor(0.7459)\n",
      "91 tensor(0.7456)\n",
      "92 tensor(0.7452)\n",
      "93 tensor(0.7449)\n",
      "94 tensor(0.7446)\n",
      "95 tensor(0.7442)\n",
      "96 tensor(0.7439)\n",
      "97 tensor(0.7435)\n",
      "98 tensor(0.7432)\n",
      "99 tensor(0.7428)\n",
      "100 tensor(0.7425)\n",
      "101 tensor(0.7422)\n",
      "102 tensor(0.7418)\n",
      "103 tensor(0.7415)\n",
      "104 tensor(0.7411)\n",
      "105 tensor(0.7408)\n",
      "106 tensor(0.7405)\n",
      "107 tensor(0.7401)\n",
      "108 tensor(0.7398)\n",
      "109 tensor(0.7395)\n",
      "110 tensor(0.7391)\n",
      "111 tensor(0.7388)\n",
      "112 tensor(0.7385)\n",
      "113 tensor(0.7381)\n",
      "114 tensor(0.7378)\n",
      "115 tensor(0.7374)\n",
      "116 tensor(0.7371)\n",
      "117 tensor(0.7368)\n",
      "118 tensor(0.7364)\n",
      "119 tensor(0.7361)\n",
      "120 tensor(0.7358)\n",
      "121 tensor(0.7354)\n",
      "122 tensor(0.7351)\n",
      "123 tensor(0.7348)\n",
      "124 tensor(0.7345)\n",
      "125 tensor(0.7341)\n",
      "126 tensor(0.7338)\n",
      "127 tensor(0.7335)\n",
      "128 tensor(0.7331)\n",
      "129 tensor(0.7328)\n",
      "130 tensor(0.7325)\n",
      "131 tensor(0.7321)\n",
      "132 tensor(0.7318)\n",
      "133 tensor(0.7315)\n",
      "134 tensor(0.7312)\n",
      "135 tensor(0.7308)\n",
      "136 tensor(0.7305)\n",
      "137 tensor(0.7302)\n",
      "138 tensor(0.7298)\n",
      "139 tensor(0.7295)\n",
      "140 tensor(0.7292)\n",
      "141 tensor(0.7289)\n",
      "142 tensor(0.7285)\n",
      "143 tensor(0.7282)\n",
      "144 tensor(0.7279)\n",
      "145 tensor(0.7276)\n",
      "146 tensor(0.7272)\n",
      "147 tensor(0.7269)\n",
      "148 tensor(0.7266)\n",
      "149 tensor(0.7263)\n",
      "150 tensor(0.7259)\n",
      "151 tensor(0.7256)\n",
      "152 tensor(0.7253)\n",
      "153 tensor(0.7250)\n",
      "154 tensor(0.7246)\n",
      "155 tensor(0.7243)\n",
      "156 tensor(0.7240)\n",
      "157 tensor(0.7237)\n",
      "158 tensor(0.7233)\n",
      "159 tensor(0.7230)\n",
      "160 tensor(0.7227)\n",
      "161 tensor(0.7224)\n",
      "162 tensor(0.7221)\n",
      "163 tensor(0.7217)\n",
      "164 tensor(0.7214)\n",
      "165 tensor(0.7211)\n",
      "166 tensor(0.7208)\n",
      "167 tensor(0.7204)\n",
      "168 tensor(0.7201)\n",
      "169 tensor(0.7198)\n",
      "170 tensor(0.7195)\n",
      "171 tensor(0.7192)\n",
      "172 tensor(0.7188)\n",
      "173 tensor(0.7185)\n",
      "174 tensor(0.7182)\n",
      "175 tensor(0.7179)\n",
      "176 tensor(0.7176)\n",
      "177 tensor(0.7173)\n",
      "178 tensor(0.7169)\n",
      "179 tensor(0.7166)\n",
      "180 tensor(0.7163)\n",
      "181 tensor(0.7160)\n",
      "182 tensor(0.7157)\n",
      "183 tensor(0.7153)\n",
      "184 tensor(0.7150)\n",
      "185 tensor(0.7147)\n",
      "186 tensor(0.7144)\n",
      "187 tensor(0.7141)\n",
      "188 tensor(0.7138)\n",
      "189 tensor(0.7135)\n",
      "190 tensor(0.7131)\n",
      "191 tensor(0.7128)\n",
      "192 tensor(0.7125)\n",
      "193 tensor(0.7122)\n",
      "194 tensor(0.7119)\n",
      "195 tensor(0.7116)\n",
      "196 tensor(0.7112)\n",
      "197 tensor(0.7109)\n",
      "198 tensor(0.7106)\n",
      "199 tensor(0.7103)\n",
      "200 tensor(0.7100)\n",
      "201 tensor(0.7097)\n",
      "202 tensor(0.7094)\n",
      "203 tensor(0.7091)\n",
      "204 tensor(0.7087)\n",
      "205 tensor(0.7084)\n",
      "206 tensor(0.7081)\n",
      "207 tensor(0.7078)\n",
      "208 tensor(0.7075)\n",
      "209 tensor(0.7072)\n",
      "210 tensor(0.7069)\n",
      "211 tensor(0.7066)\n",
      "212 tensor(0.7063)\n",
      "213 tensor(0.7059)\n",
      "214 tensor(0.7056)\n",
      "215 tensor(0.7053)\n",
      "216 tensor(0.7050)\n",
      "217 tensor(0.7047)\n",
      "218 tensor(0.7044)\n",
      "219 tensor(0.7041)\n",
      "220 tensor(0.7038)\n",
      "221 tensor(0.7035)\n",
      "222 tensor(0.7032)\n",
      "223 tensor(0.7029)\n",
      "224 tensor(0.7025)\n",
      "225 tensor(0.7022)\n",
      "226 tensor(0.7019)\n",
      "227 tensor(0.7016)\n",
      "228 tensor(0.7013)\n",
      "229 tensor(0.7010)\n",
      "230 tensor(0.7007)\n",
      "231 tensor(0.7004)\n",
      "232 tensor(0.7001)\n",
      "233 tensor(0.6998)\n",
      "234 tensor(0.6995)\n",
      "235 tensor(0.6992)\n",
      "236 tensor(0.6989)\n",
      "237 tensor(0.6986)\n",
      "238 tensor(0.6983)\n",
      "239 tensor(0.6979)\n",
      "240 tensor(0.6976)\n",
      "241 tensor(0.6973)\n",
      "242 tensor(0.6970)\n",
      "243 tensor(0.6967)\n",
      "244 tensor(0.6964)\n",
      "245 tensor(0.6961)\n",
      "246 tensor(0.6958)\n",
      "247 tensor(0.6955)\n",
      "248 tensor(0.6952)\n",
      "249 tensor(0.6949)\n",
      "250 tensor(0.6946)\n",
      "251 tensor(0.6943)\n",
      "252 tensor(0.6940)\n",
      "253 tensor(0.6937)\n",
      "254 tensor(0.6934)\n",
      "255 tensor(0.6931)\n",
      "256 tensor(0.6928)\n",
      "257 tensor(0.6925)\n",
      "258 tensor(0.6922)\n",
      "259 tensor(0.6919)\n",
      "260 tensor(0.6916)\n",
      "261 tensor(0.6913)\n",
      "262 tensor(0.6910)\n",
      "263 tensor(0.6907)\n",
      "264 tensor(0.6904)\n",
      "265 tensor(0.6901)\n",
      "266 tensor(0.6898)\n",
      "267 tensor(0.6895)\n",
      "268 tensor(0.6892)\n",
      "269 tensor(0.6889)\n",
      "270 tensor(0.6886)\n",
      "271 tensor(0.6883)\n",
      "272 tensor(0.6880)\n",
      "273 tensor(0.6877)\n",
      "274 tensor(0.6874)\n",
      "275 tensor(0.6871)\n",
      "276 tensor(0.6868)\n",
      "277 tensor(0.6865)\n",
      "278 tensor(0.6862)\n",
      "279 tensor(0.6859)\n",
      "280 tensor(0.6856)\n",
      "281 tensor(0.6853)\n",
      "282 tensor(0.6850)\n",
      "283 tensor(0.6847)\n",
      "284 tensor(0.6844)\n",
      "285 tensor(0.6841)\n",
      "286 tensor(0.6839)\n",
      "287 tensor(0.6836)\n",
      "288 tensor(0.6833)\n",
      "289 tensor(0.6830)\n",
      "290 tensor(0.6827)\n",
      "291 tensor(0.6824)\n",
      "292 tensor(0.6821)\n",
      "293 tensor(0.6818)\n",
      "294 tensor(0.6815)\n",
      "295 tensor(0.6812)\n",
      "296 tensor(0.6809)\n",
      "297 tensor(0.6806)\n",
      "298 tensor(0.6803)\n",
      "299 tensor(0.6800)\n",
      "300 tensor(0.6797)\n",
      "301 tensor(0.6794)\n",
      "302 tensor(0.6792)\n",
      "303 tensor(0.6789)\n",
      "304 tensor(0.6786)\n",
      "305 tensor(0.6783)\n",
      "306 tensor(0.6780)\n",
      "307 tensor(0.6777)\n",
      "308 tensor(0.6774)\n",
      "309 tensor(0.6771)\n",
      "310 tensor(0.6768)\n",
      "311 tensor(0.6765)\n",
      "312 tensor(0.6762)\n",
      "313 tensor(0.6760)\n",
      "314 tensor(0.6757)\n",
      "315 tensor(0.6754)\n",
      "316 tensor(0.6751)\n",
      "317 tensor(0.6748)\n",
      "318 tensor(0.6745)\n",
      "319 tensor(0.6742)\n",
      "320 tensor(0.6739)\n",
      "321 tensor(0.6736)\n",
      "322 tensor(0.6734)\n",
      "323 tensor(0.6731)\n",
      "324 tensor(0.6728)\n",
      "325 tensor(0.6725)\n",
      "326 tensor(0.6722)\n",
      "327 tensor(0.6719)\n",
      "328 tensor(0.6716)\n",
      "329 tensor(0.6713)\n",
      "330 tensor(0.6711)\n",
      "331 tensor(0.6708)\n",
      "332 tensor(0.6705)\n",
      "333 tensor(0.6702)\n",
      "334 tensor(0.6699)\n",
      "335 tensor(0.6696)\n",
      "336 tensor(0.6693)\n",
      "337 tensor(0.6691)\n",
      "338 tensor(0.6688)\n",
      "339 tensor(0.6685)\n",
      "340 tensor(0.6682)\n",
      "341 tensor(0.6679)\n",
      "342 tensor(0.6676)\n",
      "343 tensor(0.6674)\n",
      "344 tensor(0.6671)\n",
      "345 tensor(0.6668)\n",
      "346 tensor(0.6665)\n",
      "347 tensor(0.6662)\n",
      "348 tensor(0.6659)\n",
      "349 tensor(0.6657)\n",
      "350 tensor(0.6654)\n",
      "351 tensor(0.6651)\n",
      "352 tensor(0.6648)\n",
      "353 tensor(0.6645)\n",
      "354 tensor(0.6642)\n",
      "355 tensor(0.6640)\n",
      "356 tensor(0.6637)\n",
      "357 tensor(0.6634)\n",
      "358 tensor(0.6631)\n",
      "359 tensor(0.6628)\n",
      "360 tensor(0.6626)\n",
      "361 tensor(0.6623)\n",
      "362 tensor(0.6620)\n",
      "363 tensor(0.6617)\n",
      "364 tensor(0.6614)\n",
      "365 tensor(0.6612)\n",
      "366 tensor(0.6609)\n",
      "367 tensor(0.6606)\n",
      "368 tensor(0.6603)\n",
      "369 tensor(0.6600)\n",
      "370 tensor(0.6598)\n",
      "371 tensor(0.6595)\n",
      "372 tensor(0.6592)\n",
      "373 tensor(0.6589)\n",
      "374 tensor(0.6587)\n",
      "375 tensor(0.6584)\n",
      "376 tensor(0.6581)\n",
      "377 tensor(0.6578)\n",
      "378 tensor(0.6575)\n",
      "379 tensor(0.6573)\n",
      "380 tensor(0.6570)\n",
      "381 tensor(0.6567)\n",
      "382 tensor(0.6564)\n",
      "383 tensor(0.6562)\n",
      "384 tensor(0.6559)\n",
      "385 tensor(0.6556)\n",
      "386 tensor(0.6553)\n",
      "387 tensor(0.6551)\n",
      "388 tensor(0.6548)\n",
      "389 tensor(0.6545)\n",
      "390 tensor(0.6542)\n",
      "391 tensor(0.6540)\n",
      "392 tensor(0.6537)\n",
      "393 tensor(0.6534)\n",
      "394 tensor(0.6531)\n",
      "395 tensor(0.6529)\n",
      "396 tensor(0.6526)\n",
      "397 tensor(0.6523)\n",
      "398 tensor(0.6520)\n",
      "399 tensor(0.6518)\n",
      "400 tensor(0.6515)\n",
      "401 tensor(0.6512)\n",
      "402 tensor(0.6509)\n",
      "403 tensor(0.6507)\n",
      "404 tensor(0.6504)\n",
      "405 tensor(0.6501)\n",
      "406 tensor(0.6499)\n",
      "407 tensor(0.6496)\n",
      "408 tensor(0.6493)\n",
      "409 tensor(0.6490)\n",
      "410 tensor(0.6488)\n",
      "411 tensor(0.6485)\n",
      "412 tensor(0.6482)\n",
      "413 tensor(0.6480)\n",
      "414 tensor(0.6477)\n",
      "415 tensor(0.6474)\n",
      "416 tensor(0.6472)\n",
      "417 tensor(0.6469)\n",
      "418 tensor(0.6466)\n",
      "419 tensor(0.6463)\n",
      "420 tensor(0.6461)\n",
      "421 tensor(0.6458)\n",
      "422 tensor(0.6455)\n",
      "423 tensor(0.6453)\n",
      "424 tensor(0.6450)\n",
      "425 tensor(0.6447)\n",
      "426 tensor(0.6445)\n",
      "427 tensor(0.6442)\n",
      "428 tensor(0.6439)\n",
      "429 tensor(0.6437)\n",
      "430 tensor(0.6434)\n",
      "431 tensor(0.6431)\n",
      "432 tensor(0.6429)\n",
      "433 tensor(0.6426)\n",
      "434 tensor(0.6423)\n",
      "435 tensor(0.6421)\n",
      "436 tensor(0.6418)\n",
      "437 tensor(0.6415)\n",
      "438 tensor(0.6413)\n",
      "439 tensor(0.6410)\n",
      "440 tensor(0.6407)\n",
      "441 tensor(0.6405)\n",
      "442 tensor(0.6402)\n",
      "443 tensor(0.6399)\n",
      "444 tensor(0.6397)\n",
      "445 tensor(0.6394)\n",
      "446 tensor(0.6391)\n",
      "447 tensor(0.6389)\n",
      "448 tensor(0.6386)\n",
      "449 tensor(0.6384)\n",
      "450 tensor(0.6381)\n",
      "451 tensor(0.6378)\n",
      "452 tensor(0.6376)\n",
      "453 tensor(0.6373)\n",
      "454 tensor(0.6370)\n",
      "455 tensor(0.6368)\n",
      "456 tensor(0.6365)\n",
      "457 tensor(0.6363)\n",
      "458 tensor(0.6360)\n",
      "459 tensor(0.6357)\n",
      "460 tensor(0.6355)\n",
      "461 tensor(0.6352)\n",
      "462 tensor(0.6349)\n",
      "463 tensor(0.6347)\n",
      "464 tensor(0.6344)\n",
      "465 tensor(0.6342)\n",
      "466 tensor(0.6339)\n",
      "467 tensor(0.6336)\n",
      "468 tensor(0.6334)\n",
      "469 tensor(0.6331)\n",
      "470 tensor(0.6329)\n",
      "471 tensor(0.6326)\n",
      "472 tensor(0.6323)\n",
      "473 tensor(0.6321)\n",
      "474 tensor(0.6318)\n",
      "475 tensor(0.6316)\n",
      "476 tensor(0.6313)\n",
      "477 tensor(0.6310)\n",
      "478 tensor(0.6308)\n",
      "479 tensor(0.6305)\n",
      "480 tensor(0.6303)\n",
      "481 tensor(0.6300)\n",
      "482 tensor(0.6298)\n",
      "483 tensor(0.6295)\n",
      "484 tensor(0.6292)\n",
      "485 tensor(0.6290)\n",
      "486 tensor(0.6287)\n",
      "487 tensor(0.6285)\n",
      "488 tensor(0.6282)\n",
      "489 tensor(0.6280)\n",
      "490 tensor(0.6277)\n",
      "491 tensor(0.6274)\n",
      "492 tensor(0.6272)\n",
      "493 tensor(0.6269)\n",
      "494 tensor(0.6267)\n",
      "495 tensor(0.6264)\n",
      "496 tensor(0.6262)\n",
      "497 tensor(0.6259)\n",
      "498 tensor(0.6257)\n",
      "499 tensor(0.6254)\n",
      "500 tensor(0.6251)\n",
      "501 tensor(0.6249)\n",
      "502 tensor(0.6246)\n",
      "503 tensor(0.6244)\n",
      "504 tensor(0.6241)\n",
      "505 tensor(0.6239)\n",
      "506 tensor(0.6236)\n",
      "507 tensor(0.6234)\n",
      "508 tensor(0.6231)\n",
      "509 tensor(0.6229)\n",
      "510 tensor(0.6226)\n",
      "511 tensor(0.6224)\n",
      "512 tensor(0.6221)\n",
      "513 tensor(0.6219)\n",
      "514 tensor(0.6216)\n",
      "515 tensor(0.6214)\n",
      "516 tensor(0.6211)\n",
      "517 tensor(0.6208)\n",
      "518 tensor(0.6206)\n",
      "519 tensor(0.6203)\n",
      "520 tensor(0.6201)\n",
      "521 tensor(0.6198)\n",
      "522 tensor(0.6196)\n",
      "523 tensor(0.6193)\n",
      "524 tensor(0.6191)\n",
      "525 tensor(0.6188)\n",
      "526 tensor(0.6186)\n",
      "527 tensor(0.6183)\n",
      "528 tensor(0.6181)\n",
      "529 tensor(0.6178)\n",
      "530 tensor(0.6176)\n",
      "531 tensor(0.6173)\n",
      "532 tensor(0.6171)\n",
      "533 tensor(0.6168)\n",
      "534 tensor(0.6166)\n",
      "535 tensor(0.6164)\n",
      "536 tensor(0.6161)\n",
      "537 tensor(0.6159)\n",
      "538 tensor(0.6156)\n",
      "539 tensor(0.6154)\n",
      "540 tensor(0.6151)\n",
      "541 tensor(0.6149)\n",
      "542 tensor(0.6146)\n",
      "543 tensor(0.6144)\n",
      "544 tensor(0.6141)\n",
      "545 tensor(0.6139)\n",
      "546 tensor(0.6136)\n",
      "547 tensor(0.6134)\n",
      "548 tensor(0.6131)\n",
      "549 tensor(0.6129)\n",
      "550 tensor(0.6126)\n",
      "551 tensor(0.6124)\n",
      "552 tensor(0.6122)\n",
      "553 tensor(0.6119)\n",
      "554 tensor(0.6117)\n",
      "555 tensor(0.6114)\n",
      "556 tensor(0.6112)\n",
      "557 tensor(0.6109)\n",
      "558 tensor(0.6107)\n",
      "559 tensor(0.6104)\n",
      "560 tensor(0.6102)\n",
      "561 tensor(0.6100)\n",
      "562 tensor(0.6097)\n",
      "563 tensor(0.6095)\n",
      "564 tensor(0.6092)\n",
      "565 tensor(0.6090)\n",
      "566 tensor(0.6087)\n",
      "567 tensor(0.6085)\n",
      "568 tensor(0.6083)\n",
      "569 tensor(0.6080)\n",
      "570 tensor(0.6078)\n",
      "571 tensor(0.6075)\n",
      "572 tensor(0.6073)\n",
      "573 tensor(0.6070)\n",
      "574 tensor(0.6068)\n",
      "575 tensor(0.6066)\n",
      "576 tensor(0.6063)\n",
      "577 tensor(0.6061)\n",
      "578 tensor(0.6058)\n",
      "579 tensor(0.6056)\n",
      "580 tensor(0.6054)\n",
      "581 tensor(0.6051)\n",
      "582 tensor(0.6049)\n",
      "583 tensor(0.6046)\n",
      "584 tensor(0.6044)\n",
      "585 tensor(0.6042)\n",
      "586 tensor(0.6039)\n",
      "587 tensor(0.6037)\n",
      "588 tensor(0.6034)\n",
      "589 tensor(0.6032)\n",
      "590 tensor(0.6030)\n",
      "591 tensor(0.6027)\n",
      "592 tensor(0.6025)\n",
      "593 tensor(0.6022)\n",
      "594 tensor(0.6020)\n",
      "595 tensor(0.6018)\n",
      "596 tensor(0.6015)\n",
      "597 tensor(0.6013)\n",
      "598 tensor(0.6011)\n",
      "599 tensor(0.6008)\n",
      "600 tensor(0.6006)\n",
      "601 tensor(0.6003)\n",
      "602 tensor(0.6001)\n",
      "603 tensor(0.5999)\n",
      "604 tensor(0.5996)\n",
      "605 tensor(0.5994)\n",
      "606 tensor(0.5992)\n",
      "607 tensor(0.5989)\n",
      "608 tensor(0.5987)\n",
      "609 tensor(0.5984)\n",
      "610 tensor(0.5982)\n",
      "611 tensor(0.5980)\n",
      "612 tensor(0.5977)\n",
      "613 tensor(0.5975)\n",
      "614 tensor(0.5973)\n",
      "615 tensor(0.5970)\n",
      "616 tensor(0.5968)\n",
      "617 tensor(0.5966)\n",
      "618 tensor(0.5963)\n",
      "619 tensor(0.5961)\n",
      "620 tensor(0.5959)\n",
      "621 tensor(0.5956)\n",
      "622 tensor(0.5954)\n",
      "623 tensor(0.5952)\n",
      "624 tensor(0.5949)\n",
      "625 tensor(0.5947)\n",
      "626 tensor(0.5945)\n",
      "627 tensor(0.5942)\n",
      "628 tensor(0.5940)\n",
      "629 tensor(0.5938)\n",
      "630 tensor(0.5935)\n",
      "631 tensor(0.5933)\n",
      "632 tensor(0.5931)\n",
      "633 tensor(0.5928)\n",
      "634 tensor(0.5926)\n",
      "635 tensor(0.5924)\n",
      "636 tensor(0.5921)\n",
      "637 tensor(0.5919)\n",
      "638 tensor(0.5917)\n",
      "639 tensor(0.5914)\n",
      "640 tensor(0.5912)\n",
      "641 tensor(0.5910)\n",
      "642 tensor(0.5908)\n",
      "643 tensor(0.5905)\n",
      "644 tensor(0.5903)\n",
      "645 tensor(0.5901)\n",
      "646 tensor(0.5898)\n",
      "647 tensor(0.5896)\n",
      "648 tensor(0.5894)\n",
      "649 tensor(0.5891)\n",
      "650 tensor(0.5889)\n",
      "651 tensor(0.5887)\n",
      "652 tensor(0.5885)\n",
      "653 tensor(0.5882)\n",
      "654 tensor(0.5880)\n",
      "655 tensor(0.5878)\n",
      "656 tensor(0.5875)\n",
      "657 tensor(0.5873)\n",
      "658 tensor(0.5871)\n",
      "659 tensor(0.5869)\n",
      "660 tensor(0.5866)\n",
      "661 tensor(0.5864)\n",
      "662 tensor(0.5862)\n",
      "663 tensor(0.5860)\n",
      "664 tensor(0.5857)\n",
      "665 tensor(0.5855)\n",
      "666 tensor(0.5853)\n",
      "667 tensor(0.5850)\n",
      "668 tensor(0.5848)\n",
      "669 tensor(0.5846)\n",
      "670 tensor(0.5844)\n",
      "671 tensor(0.5841)\n",
      "672 tensor(0.5839)\n",
      "673 tensor(0.5837)\n",
      "674 tensor(0.5835)\n",
      "675 tensor(0.5832)\n",
      "676 tensor(0.5830)\n",
      "677 tensor(0.5828)\n",
      "678 tensor(0.5826)\n",
      "679 tensor(0.5823)\n",
      "680 tensor(0.5821)\n",
      "681 tensor(0.5819)\n",
      "682 tensor(0.5817)\n",
      "683 tensor(0.5814)\n",
      "684 tensor(0.5812)\n",
      "685 tensor(0.5810)\n",
      "686 tensor(0.5808)\n",
      "687 tensor(0.5806)\n",
      "688 tensor(0.5803)\n",
      "689 tensor(0.5801)\n",
      "690 tensor(0.5799)\n",
      "691 tensor(0.5797)\n",
      "692 tensor(0.5794)\n",
      "693 tensor(0.5792)\n",
      "694 tensor(0.5790)\n",
      "695 tensor(0.5788)\n",
      "696 tensor(0.5786)\n",
      "697 tensor(0.5783)\n",
      "698 tensor(0.5781)\n",
      "699 tensor(0.5779)\n",
      "700 tensor(0.5777)\n",
      "701 tensor(0.5774)\n",
      "702 tensor(0.5772)\n",
      "703 tensor(0.5770)\n",
      "704 tensor(0.5768)\n",
      "705 tensor(0.5766)\n",
      "706 tensor(0.5763)\n",
      "707 tensor(0.5761)\n",
      "708 tensor(0.5759)\n",
      "709 tensor(0.5757)\n",
      "710 tensor(0.5755)\n",
      "711 tensor(0.5752)\n",
      "712 tensor(0.5750)\n",
      "713 tensor(0.5748)\n",
      "714 tensor(0.5746)\n",
      "715 tensor(0.5744)\n",
      "716 tensor(0.5741)\n",
      "717 tensor(0.5739)\n",
      "718 tensor(0.5737)\n",
      "719 tensor(0.5735)\n",
      "720 tensor(0.5733)\n",
      "721 tensor(0.5731)\n",
      "722 tensor(0.5728)\n",
      "723 tensor(0.5726)\n",
      "724 tensor(0.5724)\n",
      "725 tensor(0.5722)\n",
      "726 tensor(0.5720)\n",
      "727 tensor(0.5718)\n",
      "728 tensor(0.5715)\n",
      "729 tensor(0.5713)\n",
      "730 tensor(0.5711)\n",
      "731 tensor(0.5709)\n",
      "732 tensor(0.5707)\n",
      "733 tensor(0.5705)\n",
      "734 tensor(0.5702)\n",
      "735 tensor(0.5700)\n",
      "736 tensor(0.5698)\n",
      "737 tensor(0.5696)\n",
      "738 tensor(0.5694)\n",
      "739 tensor(0.5692)\n",
      "740 tensor(0.5689)\n",
      "741 tensor(0.5687)\n",
      "742 tensor(0.5685)\n",
      "743 tensor(0.5683)\n",
      "744 tensor(0.5681)\n",
      "745 tensor(0.5679)\n",
      "746 tensor(0.5677)\n",
      "747 tensor(0.5674)\n",
      "748 tensor(0.5672)\n",
      "749 tensor(0.5670)\n",
      "750 tensor(0.5668)\n",
      "751 tensor(0.5666)\n",
      "752 tensor(0.5664)\n",
      "753 tensor(0.5662)\n",
      "754 tensor(0.5659)\n",
      "755 tensor(0.5657)\n",
      "756 tensor(0.5655)\n",
      "757 tensor(0.5653)\n",
      "758 tensor(0.5651)\n",
      "759 tensor(0.5649)\n",
      "760 tensor(0.5647)\n",
      "761 tensor(0.5645)\n",
      "762 tensor(0.5642)\n",
      "763 tensor(0.5640)\n",
      "764 tensor(0.5638)\n",
      "765 tensor(0.5636)\n",
      "766 tensor(0.5634)\n",
      "767 tensor(0.5632)\n",
      "768 tensor(0.5630)\n",
      "769 tensor(0.5628)\n",
      "770 tensor(0.5626)\n",
      "771 tensor(0.5623)\n",
      "772 tensor(0.5621)\n",
      "773 tensor(0.5619)\n",
      "774 tensor(0.5617)\n",
      "775 tensor(0.5615)\n",
      "776 tensor(0.5613)\n",
      "777 tensor(0.5611)\n",
      "778 tensor(0.5609)\n",
      "779 tensor(0.5607)\n",
      "780 tensor(0.5605)\n",
      "781 tensor(0.5602)\n",
      "782 tensor(0.5600)\n",
      "783 tensor(0.5598)\n",
      "784 tensor(0.5596)\n",
      "785 tensor(0.5594)\n",
      "786 tensor(0.5592)\n",
      "787 tensor(0.5590)\n",
      "788 tensor(0.5588)\n",
      "789 tensor(0.5586)\n",
      "790 tensor(0.5584)\n",
      "791 tensor(0.5582)\n",
      "792 tensor(0.5580)\n",
      "793 tensor(0.5578)\n",
      "794 tensor(0.5575)\n",
      "795 tensor(0.5573)\n",
      "796 tensor(0.5571)\n",
      "797 tensor(0.5569)\n",
      "798 tensor(0.5567)\n",
      "799 tensor(0.5565)\n",
      "800 tensor(0.5563)\n",
      "801 tensor(0.5561)\n",
      "802 tensor(0.5559)\n",
      "803 tensor(0.5557)\n",
      "804 tensor(0.5555)\n",
      "805 tensor(0.5553)\n",
      "806 tensor(0.5551)\n",
      "807 tensor(0.5549)\n",
      "808 tensor(0.5547)\n",
      "809 tensor(0.5545)\n",
      "810 tensor(0.5542)\n",
      "811 tensor(0.5540)\n",
      "812 tensor(0.5538)\n",
      "813 tensor(0.5536)\n",
      "814 tensor(0.5534)\n",
      "815 tensor(0.5532)\n",
      "816 tensor(0.5530)\n",
      "817 tensor(0.5528)\n",
      "818 tensor(0.5526)\n",
      "819 tensor(0.5524)\n",
      "820 tensor(0.5522)\n",
      "821 tensor(0.5520)\n",
      "822 tensor(0.5518)\n",
      "823 tensor(0.5516)\n",
      "824 tensor(0.5514)\n",
      "825 tensor(0.5512)\n",
      "826 tensor(0.5510)\n",
      "827 tensor(0.5508)\n",
      "828 tensor(0.5506)\n",
      "829 tensor(0.5504)\n",
      "830 tensor(0.5502)\n",
      "831 tensor(0.5500)\n",
      "832 tensor(0.5498)\n",
      "833 tensor(0.5496)\n",
      "834 tensor(0.5494)\n",
      "835 tensor(0.5492)\n",
      "836 tensor(0.5490)\n",
      "837 tensor(0.5488)\n",
      "838 tensor(0.5486)\n",
      "839 tensor(0.5484)\n",
      "840 tensor(0.5482)\n",
      "841 tensor(0.5480)\n",
      "842 tensor(0.5478)\n",
      "843 tensor(0.5476)\n",
      "844 tensor(0.5474)\n",
      "845 tensor(0.5472)\n",
      "846 tensor(0.5470)\n",
      "847 tensor(0.5468)\n",
      "848 tensor(0.5466)\n",
      "849 tensor(0.5464)\n",
      "850 tensor(0.5462)\n",
      "851 tensor(0.5460)\n",
      "852 tensor(0.5458)\n",
      "853 tensor(0.5456)\n",
      "854 tensor(0.5454)\n",
      "855 tensor(0.5452)\n",
      "856 tensor(0.5450)\n",
      "857 tensor(0.5448)\n",
      "858 tensor(0.5446)\n",
      "859 tensor(0.5444)\n",
      "860 tensor(0.5442)\n",
      "861 tensor(0.5440)\n",
      "862 tensor(0.5438)\n",
      "863 tensor(0.5436)\n",
      "864 tensor(0.5434)\n",
      "865 tensor(0.5432)\n",
      "866 tensor(0.5430)\n",
      "867 tensor(0.5428)\n",
      "868 tensor(0.5426)\n",
      "869 tensor(0.5424)\n",
      "870 tensor(0.5422)\n",
      "871 tensor(0.5420)\n",
      "872 tensor(0.5418)\n",
      "873 tensor(0.5416)\n",
      "874 tensor(0.5414)\n",
      "875 tensor(0.5412)\n",
      "876 tensor(0.5410)\n",
      "877 tensor(0.5408)\n",
      "878 tensor(0.5406)\n",
      "879 tensor(0.5404)\n",
      "880 tensor(0.5403)\n",
      "881 tensor(0.5401)\n",
      "882 tensor(0.5399)\n",
      "883 tensor(0.5397)\n",
      "884 tensor(0.5395)\n",
      "885 tensor(0.5393)\n",
      "886 tensor(0.5391)\n",
      "887 tensor(0.5389)\n",
      "888 tensor(0.5387)\n",
      "889 tensor(0.5385)\n",
      "890 tensor(0.5383)\n",
      "891 tensor(0.5381)\n",
      "892 tensor(0.5379)\n",
      "893 tensor(0.5377)\n",
      "894 tensor(0.5375)\n",
      "895 tensor(0.5373)\n",
      "896 tensor(0.5371)\n",
      "897 tensor(0.5370)\n",
      "898 tensor(0.5368)\n",
      "899 tensor(0.5366)\n",
      "900 tensor(0.5364)\n",
      "901 tensor(0.5362)\n",
      "902 tensor(0.5360)\n",
      "903 tensor(0.5358)\n",
      "904 tensor(0.5356)\n",
      "905 tensor(0.5354)\n",
      "906 tensor(0.5352)\n",
      "907 tensor(0.5350)\n",
      "908 tensor(0.5348)\n",
      "909 tensor(0.5347)\n",
      "910 tensor(0.5345)\n",
      "911 tensor(0.5343)\n",
      "912 tensor(0.5341)\n",
      "913 tensor(0.5339)\n",
      "914 tensor(0.5337)\n",
      "915 tensor(0.5335)\n",
      "916 tensor(0.5333)\n",
      "917 tensor(0.5331)\n",
      "918 tensor(0.5329)\n",
      "919 tensor(0.5327)\n",
      "920 tensor(0.5326)\n",
      "921 tensor(0.5324)\n",
      "922 tensor(0.5322)\n",
      "923 tensor(0.5320)\n",
      "924 tensor(0.5318)\n",
      "925 tensor(0.5316)\n",
      "926 tensor(0.5314)\n",
      "927 tensor(0.5312)\n",
      "928 tensor(0.5310)\n",
      "929 tensor(0.5309)\n",
      "930 tensor(0.5307)\n",
      "931 tensor(0.5305)\n",
      "932 tensor(0.5303)\n",
      "933 tensor(0.5301)\n",
      "934 tensor(0.5299)\n",
      "935 tensor(0.5297)\n",
      "936 tensor(0.5295)\n",
      "937 tensor(0.5293)\n",
      "938 tensor(0.5292)\n",
      "939 tensor(0.5290)\n",
      "940 tensor(0.5288)\n",
      "941 tensor(0.5286)\n",
      "942 tensor(0.5284)\n",
      "943 tensor(0.5282)\n",
      "944 tensor(0.5280)\n",
      "945 tensor(0.5279)\n",
      "946 tensor(0.5277)\n",
      "947 tensor(0.5275)\n",
      "948 tensor(0.5273)\n",
      "949 tensor(0.5271)\n",
      "950 tensor(0.5269)\n",
      "951 tensor(0.5267)\n",
      "952 tensor(0.5265)\n",
      "953 tensor(0.5264)\n",
      "954 tensor(0.5262)\n",
      "955 tensor(0.5260)\n",
      "956 tensor(0.5258)\n",
      "957 tensor(0.5256)\n",
      "958 tensor(0.5254)\n",
      "959 tensor(0.5253)\n",
      "960 tensor(0.5251)\n",
      "961 tensor(0.5249)\n",
      "962 tensor(0.5247)\n",
      "963 tensor(0.5245)\n",
      "964 tensor(0.5243)\n",
      "965 tensor(0.5241)\n",
      "966 tensor(0.5240)\n",
      "967 tensor(0.5238)\n",
      "968 tensor(0.5236)\n",
      "969 tensor(0.5234)\n",
      "970 tensor(0.5232)\n",
      "971 tensor(0.5230)\n",
      "972 tensor(0.5229)\n",
      "973 tensor(0.5227)\n",
      "974 tensor(0.5225)\n",
      "975 tensor(0.5223)\n",
      "976 tensor(0.5221)\n",
      "977 tensor(0.5219)\n",
      "978 tensor(0.5218)\n",
      "979 tensor(0.5216)\n",
      "980 tensor(0.5214)\n",
      "981 tensor(0.5212)\n",
      "982 tensor(0.5210)\n",
      "983 tensor(0.5209)\n",
      "984 tensor(0.5207)\n",
      "985 tensor(0.5205)\n",
      "986 tensor(0.5203)\n",
      "987 tensor(0.5201)\n",
      "988 tensor(0.5199)\n",
      "989 tensor(0.5198)\n",
      "990 tensor(0.5196)\n",
      "991 tensor(0.5194)\n",
      "992 tensor(0.5192)\n",
      "993 tensor(0.5190)\n",
      "994 tensor(0.5189)\n",
      "995 tensor(0.5187)\n",
      "996 tensor(0.5185)\n",
      "997 tensor(0.5183)\n",
      "998 tensor(0.5181)\n",
      "999 tensor(0.5180)\n",
      "predict 1 hour  1.0 tensor(0, dtype=torch.uint8)\n",
      "predict 7 hours 7.0 tensor(1, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[1.0]]))\n",
    "print(\"predict 1 hour \", 1.0, model(hour_var).data[0][0] > 0.5)\n",
    "hour_var = Variable(torch.Tensor([[7.0]]))\n",
    "print(\"predict 7 hours\", 7.0, model(hour_var).data[0][0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07_diabetes_RogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        #self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "       # out1 = self.sigmoid(self.l1(x))\n",
    "        #out2 = self.sigmoid(self.l2(out1))\n",
    "        #y_pred = self.sigmoid(self.l3(out2))\n",
    "        \n",
    "        out1 = self.relu(self.l1(x))\n",
    "        out2 = self.relu(self.l2(out1))\n",
    "        y_pred = self.relu(self.l3(out2))\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-baceb6b13847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Compute and print loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "Criterion = torch.nn.BCELoss(size_average=True)\n",
    "#criterion = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08_1_dataset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 inputs tensor([[-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n",
      "        [-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n",
      "        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n",
      "        [-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n",
      "        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n",
      "        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],\n",
      "        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667],\n",
      "        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n",
      "        [-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n",
      "        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n",
      "        [-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n",
      "        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n",
      "        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000],\n",
      "        [-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667],\n",
      "        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n",
      "        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n",
      "        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n",
      "        [-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n",
      "        [-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n",
      "        [-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n",
      "        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n",
      "        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n",
      "        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n",
      "        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n",
      "        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n",
      "        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n",
      "        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n",
      "        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n",
      "        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n",
      "        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000],\n",
      "        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n",
      "        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 1 inputs tensor([[ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n",
      "        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n",
      "        [-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n",
      "        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n",
      "        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n",
      "        [-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n",
      "        [ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n",
      "        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n",
      "        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000],\n",
      "        [-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n",
      "        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n",
      "        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n",
      "        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n",
      "        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n",
      "        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n",
      "        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n",
      "        [-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n",
      "        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n",
      "        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n",
      "        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n",
      "        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n",
      "        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n",
      "        [ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n",
      "        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n",
      "        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n",
      "        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n",
      "        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n",
      "        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n",
      "        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
      "        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n",
      "        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000],\n",
      "        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 2 inputs tensor([[-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n",
      "        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n",
      "        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n",
      "        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n",
      "        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n",
      "        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667],\n",
      "        [-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n",
      "        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n",
      "        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n",
      "        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n",
      "        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n",
      "        [-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n",
      "        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n",
      "        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n",
      "        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n",
      "        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667],\n",
      "        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n",
      "        [-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667],\n",
      "        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n",
      "        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n",
      "        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n",
      "        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667],\n",
      "        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n",
      "        [-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n",
      "        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n",
      "        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n",
      "        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n",
      "        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n",
      "        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n",
      "        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n",
      "        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n",
      "        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "0 3 inputs tensor([[-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n",
      "        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333],\n",
      "        [-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n",
      "        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n",
      "        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n",
      "        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000],\n",
      "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
      "        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n",
      "        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n",
      "        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n",
      "        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n",
      "        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n",
      "        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n",
      "        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n",
      "        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000],\n",
      "        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n",
      "        [-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n",
      "        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n",
      "        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n",
      "        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667],\n",
      "        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333],\n",
      "        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n",
      "        [ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n",
      "        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n",
      "        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n",
      "        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n",
      "        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000],\n",
      "        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n",
      "        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n",
      "        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n",
      "        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n",
      "        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "0 4 inputs tensor([[ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n",
      "        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n",
      "        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000],\n",
      "        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n",
      "        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000],\n",
      "        [-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n",
      "        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n",
      "        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n",
      "        [-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n",
      "        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n",
      "        [-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000],\n",
      "        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n",
      "        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n",
      "        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n",
      "        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n",
      "        [ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n",
      "        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n",
      "        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n",
      "        [-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n",
      "        [-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n",
      "        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n",
      "        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n",
      "        [ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n",
      "        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667],\n",
      "        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n",
      "        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n",
      "        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n",
      "        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n",
      "        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667],\n",
      "        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n",
      "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
      "        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 5 inputs tensor([[-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000],\n",
      "        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n",
      "        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n",
      "        [-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n",
      "        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333],\n",
      "        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n",
      "        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n",
      "        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n",
      "        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667],\n",
      "        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n",
      "        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n",
      "        [-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n",
      "        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667],\n",
      "        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n",
      "        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000],\n",
      "        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n",
      "        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n",
      "        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000],\n",
      "        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n",
      "        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667],\n",
      "        [-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n",
      "        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n",
      "        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333],\n",
      "        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667],\n",
      "        [-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n",
      "        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n",
      "        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n",
      "        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n",
      "        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n",
      "        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667],\n",
      "        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n",
      "        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "0 6 inputs tensor([[-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n",
      "        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n",
      "        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n",
      "        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n",
      "        [-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n",
      "        [-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n",
      "        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n",
      "        [-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n",
      "        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n",
      "        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333],\n",
      "        [-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333],\n",
      "        [-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n",
      "        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333],\n",
      "        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000],\n",
      "        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000],\n",
      "        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667],\n",
      "        [-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n",
      "        [ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333],\n",
      "        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n",
      "        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n",
      "        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n",
      "        [-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n",
      "        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n",
      "        [ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n",
      "        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n",
      "        [-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n",
      "        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667],\n",
      "        [-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n",
      "        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n",
      "        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000],\n",
      "        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n",
      "        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "0 7 inputs tensor([[-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],\n",
      "        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000],\n",
      "        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n",
      "        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n",
      "        [ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333],\n",
      "        [-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n",
      "        [-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n",
      "        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667],\n",
      "        [-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000],\n",
      "        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n",
      "        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n",
      "        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667],\n",
      "        [-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n",
      "        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n",
      "        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667],\n",
      "        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n",
      "        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n",
      "        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n",
      "        [-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n",
      "        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n",
      "        [-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n",
      "        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n",
      "        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n",
      "        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n",
      "        [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n",
      "        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n",
      "        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n",
      "        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n",
      "        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n",
      "        [-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n",
      "        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n",
      "        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "0 8 inputs tensor([[-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n",
      "        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n",
      "        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n",
      "        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n",
      "        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n",
      "        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n",
      "        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n",
      "        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n",
      "        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n",
      "        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667],\n",
      "        [-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667],\n",
      "        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n",
      "        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n",
      "        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000],\n",
      "        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667],\n",
      "        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n",
      "        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n",
      "        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n",
      "        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n",
      "        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n",
      "        [ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333],\n",
      "        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n",
      "        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n",
      "        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n",
      "        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n",
      "        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n",
      "        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333],\n",
      "        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n",
      "        [-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n",
      "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
      "        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000],\n",
      "        [ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333]]) labels tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 9 inputs tensor([[-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n",
      "        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n",
      "        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333],\n",
      "        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n",
      "        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000],\n",
      "        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n",
      "        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n",
      "        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000],\n",
      "        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667],\n",
      "        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n",
      "        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n",
      "        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n",
      "        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n",
      "        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n",
      "        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n",
      "        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n",
      "        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n",
      "        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n",
      "        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333],\n",
      "        [-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n",
      "        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n",
      "        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n",
      "        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n",
      "        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n",
      "        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000],\n",
      "        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n",
      "        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n",
      "        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n",
      "        [ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n",
      "        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n",
      "        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000],\n",
      "        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667]]) labels tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 10 inputs tensor([[-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],\n",
      "        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n",
      "        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n",
      "        [-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n",
      "        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n",
      "        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n",
      "        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n",
      "        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n",
      "        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333],\n",
      "        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n",
      "        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n",
      "        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333],\n",
      "        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n",
      "        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n",
      "        [ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n",
      "        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000],\n",
      "        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n",
      "        [ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n",
      "        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n",
      "        [-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667],\n",
      "        [-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n",
      "        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n",
      "        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n",
      "        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n",
      "        [-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n",
      "        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n",
      "        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667],\n",
      "        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667],\n",
      "        [ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n",
      "        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n",
      "        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n",
      "        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 11 inputs tensor([[-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n",
      "        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n",
      "        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n",
      "        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n",
      "        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n",
      "        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667],\n",
      "        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n",
      "        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n",
      "        [-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n",
      "        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667],\n",
      "        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n",
      "        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n",
      "        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n",
      "        [-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n",
      "        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n",
      "        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n",
      "        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n",
      "        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000],\n",
      "        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n",
      "        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n",
      "        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333],\n",
      "        [-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n",
      "        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n",
      "        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n",
      "        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667],\n",
      "        [-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n",
      "        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n",
      "        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n",
      "        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n",
      "        [-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n",
      "        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n",
      "        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "0 12 inputs tensor([[ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n",
      "        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n",
      "        [ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n",
      "        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n",
      "        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n",
      "        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n",
      "        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n",
      "        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667],\n",
      "        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n",
      "        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n",
      "        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n",
      "        [ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000],\n",
      "        [-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n",
      "        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n",
      "        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n",
      "        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n",
      "        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667],\n",
      "        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n",
      "        [ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n",
      "        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n",
      "        [-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n",
      "        [-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n",
      "        [-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667],\n",
      "        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667],\n",
      "        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n",
      "        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n",
      "        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n",
      "        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n",
      "        [-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n",
      "        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n",
      "        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n",
      "        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 13 inputs tensor([[-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n",
      "        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333],\n",
      "        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667],\n",
      "        [ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n",
      "        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n",
      "        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n",
      "        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n",
      "        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667],\n",
      "        [ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n",
      "        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000],\n",
      "        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n",
      "        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n",
      "        [-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n",
      "        [-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n",
      "        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n",
      "        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000],\n",
      "        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n",
      "        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333],\n",
      "        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n",
      "        [-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667],\n",
      "        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n",
      "        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n",
      "        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n",
      "        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n",
      "        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n",
      "        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000],\n",
      "        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n",
      "        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n",
      "        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n",
      "        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667],\n",
      "        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n",
      "        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667]]) labels tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 14 inputs tensor([[ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n",
      "        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n",
      "        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n",
      "        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n",
      "        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n",
      "        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n",
      "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n",
      "        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n",
      "        [-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n",
      "        [-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n",
      "        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333],\n",
      "        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n",
      "        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n",
      "        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n",
      "        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667],\n",
      "        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333],\n",
      "        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n",
      "        [-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n",
      "        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n",
      "        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n",
      "        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n",
      "        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n",
      "        [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n",
      "        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n",
      "        [ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n",
      "        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n",
      "        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n",
      "        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n",
      "        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000],\n",
      "        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000],\n",
      "        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667],\n",
      "        [-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "0 15 inputs tensor([[-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n",
      "        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n",
      "        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n",
      "        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n",
      "        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333],\n",
      "        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n",
      "        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n",
      "        [-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n",
      "        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333],\n",
      "        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n",
      "        [-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n",
      "        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333],\n",
      "        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667],\n",
      "        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333],\n",
      "        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n",
      "        [-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n",
      "        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n",
      "        [-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n",
      "        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n",
      "        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667],\n",
      "        [ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n",
      "        [-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n",
      "        [-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n",
      "        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n",
      "        [ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n",
      "        [-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n",
      "        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333],\n",
      "        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n",
      "        [-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n",
      "        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333],\n",
      "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n",
      "        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "0 16 inputs tensor([[-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n",
      "        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n",
      "        [ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000],\n",
      "        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n",
      "        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n",
      "        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n",
      "        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n",
      "        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n",
      "        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667],\n",
      "        [-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n",
      "        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n",
      "        [-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n",
      "        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n",
      "        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n",
      "        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n",
      "        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n",
      "        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n",
      "        [-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n",
      "        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n",
      "        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n",
      "        [-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n",
      "        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n",
      "        [-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n",
      "        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n",
      "        [-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n",
      "        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n",
      "        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333],\n",
      "        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667],\n",
      "        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n",
      "        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n",
      "        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n",
      "        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "0 17 inputs tensor([[-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n",
      "        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n",
      "        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n",
      "        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n",
      "        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n",
      "        [-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n",
      "        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n",
      "        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n",
      "        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n",
      "        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n",
      "        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n",
      "        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n",
      "        [-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n",
      "        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000],\n",
      "        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n",
      "        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n",
      "        [-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n",
      "        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n",
      "        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n",
      "        [-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n",
      "        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n",
      "        [-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n",
      "        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n",
      "        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n",
      "        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667],\n",
      "        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n",
      "        [ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n",
      "        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n",
      "        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n",
      "        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n",
      "        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667],\n",
      "        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 18 inputs tensor([[-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n",
      "        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n",
      "        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n",
      "        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n",
      "        [-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n",
      "        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333],\n",
      "        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n",
      "        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],\n",
      "        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n",
      "        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n",
      "        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n",
      "        [-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n",
      "        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n",
      "        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n",
      "        [-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333],\n",
      "        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n",
      "        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n",
      "        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n",
      "        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333],\n",
      "        [-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n",
      "        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n",
      "        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n",
      "        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000],\n",
      "        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n",
      "        [-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n",
      "        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n",
      "        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000],\n",
      "        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n",
      "        [-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000],\n",
      "        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333],\n",
      "        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667],\n",
      "        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 19 inputs tensor([[-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n",
      "        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n",
      "        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n",
      "        [-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000],\n",
      "        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n",
      "        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n",
      "        [-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n",
      "        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n",
      "        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n",
      "        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n",
      "        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n",
      "        [ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n",
      "        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n",
      "        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n",
      "        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n",
      "        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n",
      "        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n",
      "        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n",
      "        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n",
      "        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n",
      "        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n",
      "        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n",
      "        [-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n",
      "        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333],\n",
      "        [-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n",
      "        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n",
      "        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n",
      "        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667],\n",
      "        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000],\n",
      "        [-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n",
      "        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n",
      "        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667]]) labels tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "0 20 inputs tensor([[ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n",
      "        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n",
      "        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n",
      "        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n",
      "        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n",
      "        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n",
      "        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n",
      "        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000],\n",
      "        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n",
      "        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000],\n",
      "        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n",
      "        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n",
      "        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n",
      "        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000],\n",
      "        [ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333],\n",
      "        [-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n",
      "        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000],\n",
      "        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n",
      "        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n",
      "        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n",
      "        [-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n",
      "        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000],\n",
      "        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n",
      "        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n",
      "        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667],\n",
      "        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333],\n",
      "        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667],\n",
      "        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n",
      "        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n",
      "        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n",
      "        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n",
      "        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "0 21 inputs tensor([[-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n",
      "        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n",
      "        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n",
      "        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n",
      "        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n",
      "        [-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n",
      "        [ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333],\n",
      "        [-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667],\n",
      "        [-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n",
      "        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n",
      "        [-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n",
      "        [-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n",
      "        [-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n",
      "        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n",
      "        [-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n",
      "        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000],\n",
      "        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000],\n",
      "        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n",
      "        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n",
      "        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667],\n",
      "        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667],\n",
      "        [ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n",
      "        [-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n",
      "        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n",
      "        [ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n",
      "        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n",
      "        [-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n",
      "        [ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n",
      "        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n",
      "        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n",
      "        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n",
      "        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333]]) labels tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "0 22 inputs tensor([[-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n",
      "        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n",
      "        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333],\n",
      "        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n",
      "        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333],\n",
      "        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n",
      "        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n",
      "        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n",
      "        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n",
      "        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000],\n",
      "        [ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n",
      "        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n",
      "        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n",
      "        [-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n",
      "        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n",
      "        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n",
      "        [-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333],\n",
      "        [-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n",
      "        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n",
      "        [ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333],\n",
      "        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n",
      "        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n",
      "        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n",
      "        [-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n",
      "        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n",
      "        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n",
      "        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n",
      "        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667],\n",
      "        [-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n",
      "        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333],\n",
      "        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n",
      "        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "0 23 inputs tensor([[-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n",
      "        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n",
      "        [-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n",
      "        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n",
      "        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n",
      "        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n",
      "        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n",
      "        [-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000],\n",
      "        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n",
      "        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n",
      "        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n",
      "        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n",
      "        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n",
      "        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n",
      "        [-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n",
      "        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n",
      "        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n",
      "        [-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333],\n",
      "        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n",
      "        [-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n",
      "        [-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n",
      "        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n",
      "        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 0 inputs tensor([[ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n",
      "        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n",
      "        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n",
      "        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n",
      "        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n",
      "        [-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n",
      "        [-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n",
      "        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667],\n",
      "        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n",
      "        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n",
      "        [-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n",
      "        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n",
      "        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n",
      "        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333],\n",
      "        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n",
      "        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n",
      "        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333],\n",
      "        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n",
      "        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n",
      "        [-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n",
      "        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n",
      "        [-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n",
      "        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n",
      "        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n",
      "        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n",
      "        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n",
      "        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n",
      "        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000],\n",
      "        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n",
      "        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333],\n",
      "        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667],\n",
      "        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333]]) labels tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "1 1 inputs tensor([[-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n",
      "        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n",
      "        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n",
      "        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n",
      "        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000],\n",
      "        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000],\n",
      "        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n",
      "        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n",
      "        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n",
      "        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n",
      "        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333],\n",
      "        [-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n",
      "        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n",
      "        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n",
      "        [-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000],\n",
      "        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n",
      "        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n",
      "        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333],\n",
      "        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n",
      "        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n",
      "        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n",
      "        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n",
      "        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000],\n",
      "        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n",
      "        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n",
      "        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n",
      "        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667],\n",
      "        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n",
      "        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n",
      "        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n",
      "        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n",
      "        [-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "1 2 inputs tensor([[-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n",
      "        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n",
      "        [-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n",
      "        [-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n",
      "        [-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n",
      "        [-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n",
      "        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n",
      "        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n",
      "        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n",
      "        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n",
      "        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n",
      "        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667],\n",
      "        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n",
      "        [-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n",
      "        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n",
      "        [-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667],\n",
      "        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n",
      "        [ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n",
      "        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n",
      "        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667],\n",
      "        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n",
      "        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n",
      "        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667],\n",
      "        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667],\n",
      "        [-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n",
      "        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n",
      "        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n",
      "        [ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n",
      "        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n",
      "        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n",
      "        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n",
      "        [ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "1 3 inputs tensor([[-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n",
      "        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n",
      "        [-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n",
      "        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000],\n",
      "        [-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n",
      "        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n",
      "        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n",
      "        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667],\n",
      "        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n",
      "        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333],\n",
      "        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n",
      "        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n",
      "        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667],\n",
      "        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n",
      "        [ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n",
      "        [-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667],\n",
      "        [ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n",
      "        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333],\n",
      "        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000],\n",
      "        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333],\n",
      "        [ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n",
      "        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n",
      "        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n",
      "        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n",
      "        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n",
      "        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n",
      "        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n",
      "        [ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n",
      "        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n",
      "        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n",
      "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
      "        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "1 4 inputs tensor([[ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n",
      "        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n",
      "        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n",
      "        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n",
      "        [-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n",
      "        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n",
      "        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n",
      "        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n",
      "        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667],\n",
      "        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000],\n",
      "        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000],\n",
      "        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n",
      "        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n",
      "        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333],\n",
      "        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n",
      "        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n",
      "        [-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n",
      "        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n",
      "        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n",
      "        [-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n",
      "        [-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n",
      "        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333],\n",
      "        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n",
      "        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n",
      "        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n",
      "        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333],\n",
      "        [ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n",
      "        [-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n",
      "        [ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n",
      "        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667],\n",
      "        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n",
      "        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 5 inputs tensor([[-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n",
      "        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n",
      "        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n",
      "        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n",
      "        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n",
      "        [-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n",
      "        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333],\n",
      "        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n",
      "        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n",
      "        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n",
      "        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667],\n",
      "        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n",
      "        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n",
      "        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n",
      "        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667],\n",
      "        [-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n",
      "        [-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n",
      "        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n",
      "        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n",
      "        [ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n",
      "        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n",
      "        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n",
      "        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n",
      "        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n",
      "        [-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n",
      "        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n",
      "        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n",
      "        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n",
      "        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667],\n",
      "        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n",
      "        [ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n",
      "        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 6 inputs tensor([[-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n",
      "        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n",
      "        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000],\n",
      "        [-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n",
      "        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n",
      "        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333],\n",
      "        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n",
      "        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n",
      "        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n",
      "        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n",
      "        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n",
      "        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n",
      "        [-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333],\n",
      "        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n",
      "        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n",
      "        [-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n",
      "        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667],\n",
      "        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n",
      "        [-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n",
      "        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333],\n",
      "        [-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n",
      "        [-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n",
      "        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667],\n",
      "        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667],\n",
      "        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n",
      "        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n",
      "        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n",
      "        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n",
      "        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n",
      "        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n",
      "        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n",
      "        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 7 inputs tensor([[-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n",
      "        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333],\n",
      "        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n",
      "        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667],\n",
      "        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n",
      "        [-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n",
      "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n",
      "        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n",
      "        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n",
      "        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333],\n",
      "        [-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n",
      "        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n",
      "        [-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n",
      "        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n",
      "        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n",
      "        [-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n",
      "        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n",
      "        [-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n",
      "        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667],\n",
      "        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n",
      "        [-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n",
      "        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667],\n",
      "        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n",
      "        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n",
      "        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n",
      "        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333],\n",
      "        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n",
      "        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n",
      "        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n",
      "        [-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n",
      "        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n",
      "        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "1 8 inputs tensor([[-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n",
      "        [-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667],\n",
      "        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n",
      "        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n",
      "        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n",
      "        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n",
      "        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n",
      "        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n",
      "        [-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n",
      "        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667],\n",
      "        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n",
      "        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000],\n",
      "        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n",
      "        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n",
      "        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n",
      "        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n",
      "        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n",
      "        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667],\n",
      "        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n",
      "        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n",
      "        [-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n",
      "        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000],\n",
      "        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n",
      "        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n",
      "        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n",
      "        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000],\n",
      "        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n",
      "        [ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n",
      "        [-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n",
      "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n",
      "        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n",
      "        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "1 9 inputs tensor([[-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n",
      "        [-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n",
      "        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000],\n",
      "        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n",
      "        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667],\n",
      "        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n",
      "        [-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n",
      "        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n",
      "        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333],\n",
      "        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667],\n",
      "        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n",
      "        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n",
      "        [-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n",
      "        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n",
      "        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667],\n",
      "        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n",
      "        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000],\n",
      "        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n",
      "        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n",
      "        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n",
      "        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333],\n",
      "        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000],\n",
      "        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n",
      "        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n",
      "        [ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333],\n",
      "        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n",
      "        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n",
      "        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n",
      "        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n",
      "        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n",
      "        [-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n",
      "        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 10 inputs tensor([[-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n",
      "        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n",
      "        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n",
      "        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n",
      "        [-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n",
      "        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n",
      "        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n",
      "        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n",
      "        [ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333],\n",
      "        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n",
      "        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n",
      "        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n",
      "        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n",
      "        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n",
      "        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n",
      "        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n",
      "        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n",
      "        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n",
      "        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667],\n",
      "        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n",
      "        [-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n",
      "        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n",
      "        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667],\n",
      "        [-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667],\n",
      "        [ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n",
      "        [-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667],\n",
      "        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667],\n",
      "        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n",
      "        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n",
      "        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n",
      "        [-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667],\n",
      "        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667]]) labels tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 11 inputs tensor([[-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000],\n",
      "        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667],\n",
      "        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n",
      "        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n",
      "        [-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n",
      "        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n",
      "        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n",
      "        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n",
      "        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n",
      "        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n",
      "        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n",
      "        [-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n",
      "        [ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n",
      "        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n",
      "        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n",
      "        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n",
      "        [ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n",
      "        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n",
      "        [-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n",
      "        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333],\n",
      "        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n",
      "        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n",
      "        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n",
      "        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n",
      "        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n",
      "        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333],\n",
      "        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333],\n",
      "        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n",
      "        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n",
      "        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n",
      "        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n",
      "        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "1 12 inputs tensor([[-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n",
      "        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n",
      "        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667],\n",
      "        [-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333],\n",
      "        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n",
      "        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n",
      "        [-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n",
      "        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n",
      "        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000],\n",
      "        [-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n",
      "        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n",
      "        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n",
      "        [-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n",
      "        [-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n",
      "        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n",
      "        [-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n",
      "        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n",
      "        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n",
      "        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n",
      "        [-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n",
      "        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n",
      "        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n",
      "        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n",
      "        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n",
      "        [-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n",
      "        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n",
      "        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n",
      "        [-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n",
      "        [ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333],\n",
      "        [-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n",
      "        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n",
      "        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "1 13 inputs tensor([[ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n",
      "        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n",
      "        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n",
      "        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n",
      "        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n",
      "        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n",
      "        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n",
      "        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],\n",
      "        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n",
      "        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n",
      "        [ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333],\n",
      "        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n",
      "        [ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n",
      "        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n",
      "        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n",
      "        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n",
      "        [-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000],\n",
      "        [-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n",
      "        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n",
      "        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n",
      "        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n",
      "        [-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n",
      "        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n",
      "        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n",
      "        [ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n",
      "        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n",
      "        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n",
      "        [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n",
      "        [-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n",
      "        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n",
      "        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n",
      "        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "1 14 inputs tensor([[-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n",
      "        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n",
      "        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n",
      "        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n",
      "        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n",
      "        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000],\n",
      "        [-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n",
      "        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n",
      "        [-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n",
      "        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n",
      "        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n",
      "        [ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n",
      "        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000],\n",
      "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
      "        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n",
      "        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n",
      "        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000],\n",
      "        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n",
      "        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n",
      "        [ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333],\n",
      "        [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n",
      "        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n",
      "        [-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n",
      "        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n",
      "        [-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667],\n",
      "        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n",
      "        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n",
      "        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667],\n",
      "        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333],\n",
      "        [-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n",
      "        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n",
      "        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667]]) labels tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 15 inputs tensor([[-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000],\n",
      "        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n",
      "        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n",
      "        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n",
      "        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n",
      "        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n",
      "        [-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n",
      "        [-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n",
      "        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333],\n",
      "        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n",
      "        [-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n",
      "        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n",
      "        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n",
      "        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n",
      "        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n",
      "        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n",
      "        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n",
      "        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],\n",
      "        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n",
      "        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333],\n",
      "        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n",
      "        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n",
      "        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000],\n",
      "        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n",
      "        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n",
      "        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n",
      "        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n",
      "        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000],\n",
      "        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n",
      "        [-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n",
      "        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000],\n",
      "        [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667]]) labels tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "1 16 inputs tensor([[ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n",
      "        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667],\n",
      "        [-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n",
      "        [-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n",
      "        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n",
      "        [ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n",
      "        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n",
      "        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n",
      "        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n",
      "        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n",
      "        [ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n",
      "        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n",
      "        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667],\n",
      "        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667],\n",
      "        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n",
      "        [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n",
      "        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n",
      "        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n",
      "        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n",
      "        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n",
      "        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n",
      "        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n",
      "        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n",
      "        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n",
      "        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000],\n",
      "        [-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n",
      "        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n",
      "        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n",
      "        [-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n",
      "        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n",
      "        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n",
      "        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 17 inputs tensor([[-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n",
      "        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n",
      "        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n",
      "        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n",
      "        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667],\n",
      "        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n",
      "        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000],\n",
      "        [-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n",
      "        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333],\n",
      "        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n",
      "        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667],\n",
      "        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n",
      "        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n",
      "        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667],\n",
      "        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n",
      "        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n",
      "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
      "        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667],\n",
      "        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n",
      "        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n",
      "        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n",
      "        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n",
      "        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n",
      "        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],\n",
      "        [-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000],\n",
      "        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n",
      "        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n",
      "        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n",
      "        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n",
      "        [-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667],\n",
      "        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n",
      "        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 18 inputs tensor([[-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n",
      "        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n",
      "        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n",
      "        [-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n",
      "        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000],\n",
      "        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n",
      "        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n",
      "        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n",
      "        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n",
      "        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n",
      "        [-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n",
      "        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n",
      "        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n",
      "        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n",
      "        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n",
      "        [-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n",
      "        [-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n",
      "        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667],\n",
      "        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667],\n",
      "        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n",
      "        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n",
      "        [-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333],\n",
      "        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n",
      "        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n",
      "        [-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n",
      "        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n",
      "        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n",
      "        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n",
      "        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333],\n",
      "        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n",
      "        [-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n",
      "        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 19 inputs tensor([[-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n",
      "        [-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n",
      "        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000],\n",
      "        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n",
      "        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n",
      "        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n",
      "        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n",
      "        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n",
      "        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333],\n",
      "        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n",
      "        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n",
      "        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n",
      "        [ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n",
      "        [-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n",
      "        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333],\n",
      "        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n",
      "        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n",
      "        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n",
      "        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n",
      "        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333],\n",
      "        [-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n",
      "        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n",
      "        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n",
      "        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n",
      "        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000],\n",
      "        [-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n",
      "        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n",
      "        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n",
      "        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n",
      "        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n",
      "        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n",
      "        [ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000]]) labels tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "1 20 inputs tensor([[-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000],\n",
      "        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n",
      "        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n",
      "        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000],\n",
      "        [-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n",
      "        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n",
      "        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n",
      "        [-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n",
      "        [ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n",
      "        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667],\n",
      "        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n",
      "        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n",
      "        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n",
      "        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333],\n",
      "        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n",
      "        [-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n",
      "        [-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n",
      "        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n",
      "        [-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n",
      "        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n",
      "        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n",
      "        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n",
      "        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667],\n",
      "        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n",
      "        [ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333],\n",
      "        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n",
      "        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n",
      "        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n",
      "        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n",
      "        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n",
      "        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n",
      "        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "1 21 inputs tensor([[-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n",
      "        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n",
      "        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n",
      "        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000],\n",
      "        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000],\n",
      "        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n",
      "        [ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333],\n",
      "        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333],\n",
      "        [-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n",
      "        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n",
      "        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n",
      "        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n",
      "        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000],\n",
      "        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333],\n",
      "        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667],\n",
      "        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667],\n",
      "        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n",
      "        [-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n",
      "        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n",
      "        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n",
      "        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333],\n",
      "        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667],\n",
      "        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n",
      "        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n",
      "        [ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n",
      "        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n",
      "        [-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n",
      "        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n",
      "        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n",
      "        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n",
      "        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n",
      "        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000]]) labels tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "1 22 inputs tensor([[-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n",
      "        [-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n",
      "        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n",
      "        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n",
      "        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n",
      "        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667],\n",
      "        [-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n",
      "        [-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n",
      "        [-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n",
      "        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n",
      "        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n",
      "        [-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n",
      "        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n",
      "        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n",
      "        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n",
      "        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n",
      "        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n",
      "        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n",
      "        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n",
      "        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000],\n",
      "        [-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n",
      "        [ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n",
      "        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n",
      "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n",
      "        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n",
      "        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n",
      "        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n",
      "        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n",
      "        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000],\n",
      "        [-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n",
      "        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000],\n",
      "        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667]]) labels tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "1 23 inputs tensor([[-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n",
      "        [-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n",
      "        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n",
      "        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n",
      "        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n",
      "        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000],\n",
      "        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333],\n",
      "        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n",
      "        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n",
      "        [-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n",
      "        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n",
      "        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n",
      "        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n",
      "        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n",
      "        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n",
      "        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667],\n",
      "        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n",
      "        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333],\n",
      "        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n",
      "        [-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n",
      "        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000],\n",
      "        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n",
      "        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000]]) labels tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "# for data view purpose\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Run your training process\n",
    "        print(epoch, i, \"inputs\", inputs.data, \"labels\", labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data[0])\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09_Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y is  tensor(0)\n",
      "PyTorch Loss1 =  tensor(0.4170) \n",
      "PyTorch Loss2= tensor(1.8406)\n",
      "Y_pred1= tensor([0])\n",
      "Y_pred2= tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Cross entropy example using one_hot encoding\n",
    "import numpy as np\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "#print(\"loss1 = \", np.sum(-Y * np.log(Y_pred1)))\n",
    "#print(\"loss2 = \", np.sum(-Y * np.log(Y_pred2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "PyTorch Loss1 =  tensor(0.4170) \n",
      "PyTorch Loss2= tensor(1.8406)\n",
      "Y_pred1= tensor([0])\n",
      "Y_pred2= tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss) not one hot encoding\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([0]), requires_grad=False)\n",
    "print(Y)\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n",
    "Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"PyTorch Loss1 = \", l1.data, \"\\nPyTorch Loss2=\", l2.data)\n",
    "\n",
    "print(\"Y_pred1=\", torch.max(Y_pred1.data, 1)[1])\n",
    "print(\"Y_pred2=\", torch.max(Y_pred2.data, 1)[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 1])\n",
      "Batch Loss1 =  tensor(0.4966) \n",
      "Batch Loss2= tensor(1.2389)\n"
     ]
    }
   ],
   "source": [
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "#Y는 각 행의 정답 위치를 알려주는 것... 보통 cross Entropy공식으로 생각하면 곤란할 듯.. 이 클래스에 따라 자기가 one hot encoding을 따로 하는듯\n",
    "Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n",
    "print(Y)\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n",
    "                                 [1.1, 0.1, 0.2],\n",
    "                                 [0.2, 2.1, 0.1]]))\n",
    "\n",
    "\n",
    "Y_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n",
    "                                 [0.2, 0.3, 0.5],\n",
    "                                 [0.2, 0.2, 0.5]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09_2_SoftMax_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307632\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.295062\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.303119\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.297354\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.302720\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.315066\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.307038\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.308602\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.299308\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.305755\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.303960\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.301395\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.304117\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.298534\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.308986\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.295814\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.304835\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.306115\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.298112\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.293057\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.290828\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.285918\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.298643\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.295748\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.291749\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.296666\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.296128\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.290302\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.285586\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.295813\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.292286\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.276556\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.287387\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.278050\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.279685\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.288285\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.296745\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.278454\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.285437\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.276155\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.286718\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.279777\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.268180\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.270669\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.272182\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.269920\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.274207\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.249538\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.264649\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.271816\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.268295\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.257617\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.241443\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.256582\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.241203\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.239325\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.240203\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.225583\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.224310\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.208819\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.204435\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.188449\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.192924\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.198312\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.186382\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.160440\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.139039\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.124226\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.103090\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.098187\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.107342\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.040526\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.056862\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.002614\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.992906\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.884620\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.797888\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.872715\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.760187\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.822283\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.611025\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.678520\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.481631\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.545534\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.610022\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.535170\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.433015\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.399385\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.283245\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.211023\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.185277\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.192171\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.305349\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.017674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0184, Accuracy: 5743/10000 (57%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.179027\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.209702\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.014983\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.238762\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.062274\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.214536\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.422569\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.931711\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.949558\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.062024\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.066345\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.036868\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.132463\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.859819\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.781555\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.978112\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.832537\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.920427\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.777410\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.812865\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.776583\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.562703\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.041501\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.052603\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.490596\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.629642\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.592250\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.653240\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.527474\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.569283\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.590183\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.853634\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.488488\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.612886\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.729482\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.530958\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.741050\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.748448\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.823177\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.661697\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.712521\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.576314\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.583893\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.650561\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.538382\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.510014\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.663918\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.830764\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.552764\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.568586\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.401653\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.613228\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.505504\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.550690\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.529180\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.655516\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.708143\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.414434\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.623488\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.693459\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.398775\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.568763\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.618193\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.549048\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.639428\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.658631\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.425098\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.522590\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.384667\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.493343\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.503736\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.565182\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.712331\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.552796\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.436543\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.383114\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.495024\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.569070\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.505201\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.468771\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.632118\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.479529\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.260189\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.238661\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.576357\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.598552\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.551288\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.449721\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.461131\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.546777\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.306364\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.400267\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.582338\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.624727\n",
      "\n",
      "Test set: Average loss: 0.0068, Accuracy: 8731/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.365052\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.461828\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.658780\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.512896\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.294075\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.299894\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.501806\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.381492\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.293636\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.344142\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.314189\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.383506\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.570442\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.387716\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.411573\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.396459\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.487745\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.251301\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.429921\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.513910\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.634627\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.399498\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.462159\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.626592\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.532908\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.384669\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.483589\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.624500\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.217704\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.178646\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.397411\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.295574\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.375892\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.317840\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.279219\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.314246\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.459665\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.485387\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.404672\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.301083\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.437214\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.409534\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.277870\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.312055\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.348771\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.157226\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.388003\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.148358\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.171580\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.754899\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.285182\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.595760\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.208306\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.329929\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.302805\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.196800\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.487083\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.349070\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.363458\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.621683\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.161328\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.334261\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.422150\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.378506\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.533264\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.550991\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.233307\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.378629\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.178794\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.410837\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.427155\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.179384\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.111729\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.415397\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.552196\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.313441\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.266464\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.320695\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.301762\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.275971\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.225043\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.172123\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.381996\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.303817\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.271294\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.524034\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.296175\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.341941\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.111669\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.464475\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.150557\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.230190\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.421965\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.168621\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 9178/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.146634\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.233218\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.232498\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.580758\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.243464\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.323038\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.211091\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.175623\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.277052\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.256655\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.295707\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.280896\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.345402\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.413298\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.233459\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.364270\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.315632\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.212412\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.248573\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.195584\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.241389\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.370135\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.253190\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.301704\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.221681\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.472712\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.213117\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.153050\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.215740\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.133116\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.214065\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.271078\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.264245\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.179184\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.207867\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.227383\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.247253\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.155029\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.314471\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.245619\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.140280\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.184256\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.173815\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.256970\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.216857\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.275178\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.520280\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.204226\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.216364\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.245996\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.371192\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.171512\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.314365\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.259615\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.324466\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.120208\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.223366\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.450856\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.327288\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.186528\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.221732\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.235458\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.268904\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.185039\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.311983\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.347583\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.192265\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.205063\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.214719\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.105596\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.304572\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.189267\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.312374\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.225091\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.368960\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.175501\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.244594\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.215103\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.197501\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.285970\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.116663\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.327663\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.083284\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.181423\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.319427\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.410800\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.220617\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.217299\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.228355\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.254198\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.130618\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.531188\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.355883\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.277928\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 9327/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.342437\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.096142\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.130864\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.155947\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.211053\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.351150\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.354577\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.134703\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.221195\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.162485\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.241487\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.142923\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.124786\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.245477\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.151702\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.184922\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.122868\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.125681\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.124004\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.147067\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.409987\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.275846\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.255108\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.231489\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.431612\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.185484\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.159074\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.132988\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.255708\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.138303\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.280091\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.239736\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.109177\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.131814\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.126630\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.444892\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.248001\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.182690\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.109747\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.387341\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.287542\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.180599\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.056805\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.128684\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.183127\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.157279\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.239460\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.136804\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.202220\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.215199\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.112031\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.130122\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.241035\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.253739\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.200961\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.207910\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.190862\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.139031\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.235057\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.206020\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.413498\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.124065\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.131019\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.197378\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.150055\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.137672\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.274791\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.139410\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.098944\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.061421\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.320150\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.195870\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.424642\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.145509\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.089858\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.151134\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.111867\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.090420\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.280160\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.302122\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.213125\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.181959\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.121494\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.146086\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.197471\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.131236\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.277554\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.113835\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.144559\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.222115\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.184307\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.167397\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.156149\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.120787\n",
      "\n",
      "Test set: Average loss: 0.0026, Accuracy: 9513/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.136469\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.179314\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.232841\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.062953\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.169135\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.240489\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.064105\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.090784\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.217697\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.096688\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.084587\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.045355\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.166720\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.180819\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.119557\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.138531\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.242632\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.169374\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.123442\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.271069\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.210238\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.195871\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.231620\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.132019\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.141684\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.165409\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.120356\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.267136\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.345861\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.166085\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.195007\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.070053\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.058432\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.107233\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.298858\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.186718\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.302989\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.144943\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.073410\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.240276\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.093598\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.064997\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.080118\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.142379\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.048328\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.166549\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.088720\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.141969\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.076893\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.054159\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.187939\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.103313\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.091146\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.088368\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.160642\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.188190\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.177106\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.188446\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.144726\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.141027\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.151373\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.121751\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.078698\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.065492\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.211091\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.055385\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.101074\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.053046\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.062920\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.368634\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.196791\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.151516\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.095814\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.053773\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.182982\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.119797\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.352629\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.202203\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.129645\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.129931\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.239725\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.162189\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.064840\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.271065\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.311251\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.071863\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.092843\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.140654\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.135861\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.235925\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.163307\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.129450\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.162891\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.087508\n",
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 9576/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.058374\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.083352\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.114323\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.081349\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.070905\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.046396\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.168528\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.127754\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.067757\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.243650\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.102502\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.028295\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.106628\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.115446\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.073976\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.142806\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.273769\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.081687\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.136758\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.181426\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.109988\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.071925\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.210584\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.130621\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.111435\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.037981\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.339570\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.122836\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.046592\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.036875\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.063069\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.125719\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.116591\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.145508\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.013685\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.115410\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.052077\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.058052\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.406867\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.219603\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.042608\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.163367\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.129569\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.080752\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.194595\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.141289\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.289507\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.123945\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.120371\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.152041\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.115104\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.081848\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.169889\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.082710\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.149054\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.050808\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.090602\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.037339\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.116101\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.055665\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.122749\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.052274\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.114076\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.124575\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.061027\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.285785\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.085585\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.037989\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.130259\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.125220\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.162912\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.076396\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.073409\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.062870\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.281642\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.049065\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.053528\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.106307\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.151556\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.133926\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.196518\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.127233\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.051303\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.065757\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.079376\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.153523\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.185645\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.080135\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.066025\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.079607\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.119655\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.197042\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.166445\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.103476\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.098504\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.064282\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.039479\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.097467\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.107212\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.162166\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.091618\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.070847\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.258724\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.092186\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.075422\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.118818\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.294551\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.116608\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.054378\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.166918\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.173973\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.143975\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.071126\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.057888\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.156010\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.078283\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.116425\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.248612\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.118884\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.061885\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.030334\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.106381\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.087719\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.121037\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.085507\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.037751\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.236546\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.057994\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.294864\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.062757\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.118767\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.056733\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.076151\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.218370\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.197677\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.028024\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.201486\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.099805\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.064836\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.054486\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.027596\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.074530\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.075424\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.184066\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.045414\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.037687\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.073186\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.154464\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.268699\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.143706\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.022814\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.246677\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.072307\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.153707\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.108276\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.101232\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.090519\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.096826\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.083154\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.166133\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.090498\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.047332\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.018579\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.054284\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.190933\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.122170\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.174682\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.046003\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.066685\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.157447\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.140929\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.128845\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.046778\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.210155\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.091793\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.066808\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.079167\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.021204\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.016533\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.047494\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.078668\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.061738\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.066169\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.091229\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.138298\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.083516\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.102316\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.048734\n",
      "\n",
      "Test set: Average loss: 0.0018, Accuracy: 9661/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.083774\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.056921\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.035787\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.114895\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.057568\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.083460\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.118796\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.053001\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.111520\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.112571\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.119521\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.053899\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.167146\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.090824\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.128203\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.188526\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.155289\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.056848\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.141788\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.053998\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.065437\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.035637\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.029782\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.047734\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.089262\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.135196\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.016393\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.079522\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.011950\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.202248\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.074160\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.190984\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.124383\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.135123\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.091952\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.099996\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.070284\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.109091\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.050359\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.016233\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.040676\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.119924\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.039547\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.022576\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.035639\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.032584\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.033167\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.027536\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.146078\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.027962\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.061765\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.106338\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.047818\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.050714\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.109999\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.217245\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.098464\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.071636\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.170759\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.177666\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.071106\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.018347\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.041657\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.069343\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.068748\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.073232\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.036029\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.029596\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.080315\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.058268\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.026792\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.109805\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.087794\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.049366\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.108267\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.058848\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.028157\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.130328\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.054512\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.075606\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.019794\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.052225\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.061927\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.027075\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.129919\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.057474\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.027767\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.082472\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.110337\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.145758\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.050105\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.048324\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.232840\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.028756\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 9675/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.2000, 0.9000, 0.1000]])\n",
      "tensor([0.9000]) tensor([2])\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "#just for torch.max test\n",
    "Y_pred_scores = Variable(torch.Tensor([[0.3, 0.2, 0.9, 0.1]]))\n",
    "print(Y_pred_scores)\n",
    "\n",
    "#torch.max returns both the max values and indices\n",
    "Y_pred_val, Y_pred_idx = torch.max(Y_pred_scores.data, 1)\n",
    "print(Y_pred_val, Y_pred_idx)\n",
    "y_pred_idx = torch.max(Y_pred_scores.data,1)[1]\n",
    "print(y_pred_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10_1_MNIST_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model.conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301609\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.292697\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.281476\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.285262\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.241161\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.244456\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.208648\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.182077\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.138679\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.054010\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.942920\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.776359\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.624144\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.345026\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.028052\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.998259\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.941431\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.829916\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.728360\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.695535\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.462962\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.515067\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.376087\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.375618\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.419285\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.473135\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.371220\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.319410\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.497391\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.317292\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.345066\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.549277\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.573226\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.436248\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.385288\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.300061\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.449723\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.622546\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.372057\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.443062\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.129525\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.309389\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.232157\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.358535\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.157006\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.208176\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.399299\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.235702\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.373050\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.313315\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.391733\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.387833\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.257966\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.194443\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.292073\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.522844\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.220627\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.245876\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.061265\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.154210\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.188816\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.388888\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.317867\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.286244\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.185795\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.298309\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.228775\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.180499\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.271001\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.170123\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.241231\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.184816\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.158619\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.465792\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.159418\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.224286\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.195790\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.244519\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.149998\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.155149\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.150867\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.178337\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.133443\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.357058\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.075536\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.257475\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.436769\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.280978\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.124827\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.257280\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.188950\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.317878\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.090292\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.090453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1808, Accuracy: 9466/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.105252\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.174871\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.119524\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.147263\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.194014\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.211439\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.125617\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.317368\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.270675\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.138950\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.171476\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.122384\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.100431\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.245362\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.288172\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.230558\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.220725\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.211584\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.297059\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.165438\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.067676\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.165744\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.118652\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.189790\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.237267\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.091122\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.162277\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.174415\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.126649\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.216683\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.152317\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.326765\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.086413\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.111829\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.133514\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.233055\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.183040\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.113618\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.092544\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.380403\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.156015\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.163857\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.049761\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.103602\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.067395\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.078738\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.067661\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.164669\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.176947\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.098174\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.151244\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.170515\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.124293\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.085879\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.090821\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.106783\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.117097\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.150379\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.101993\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.249160\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.348275\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.103888\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.081123\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.253283\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.043511\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.142788\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.160747\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.168005\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.150137\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.116419\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.192492\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.139966\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.154689\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.244896\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.031179\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.247603\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.298035\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.045403\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.053078\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.140443\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.358617\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.156073\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.123351\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.082333\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.090107\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.110129\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.178809\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.100469\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.210106\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.137137\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.087934\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.104859\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.100554\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.228645\n",
      "\n",
      "Test set: Average loss: 0.1152, Accuracy: 9661/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.079746\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.147929\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.217842\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.191406\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.051694\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.174371\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.090167\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.171273\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.060947\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.149815\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.094379\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.183179\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.088719\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.249987\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.114815\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.136671\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.162645\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.032619\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.066163\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.215655\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.047495\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.115299\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.196018\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.222839\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.133616\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.157665\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.049003\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.052132\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.098762\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.116612\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.143147\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.180488\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.057601\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.117602\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.258113\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.090860\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.200681\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.096916\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.150639\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.275405\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.207982\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.127693\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.065593\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.231084\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.032756\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.030422\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.162806\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.120939\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.047201\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.113685\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.034979\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.044747\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.237105\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.138457\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.060118\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.213893\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.057762\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.135024\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.023419\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.153019\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.060961\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.139956\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.272206\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.035476\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.084036\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.037982\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.065850\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.017154\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.067823\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.234580\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.080669\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.027781\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.130054\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.043873\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.103418\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.051722\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.062751\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.046630\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.070098\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.062558\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.068416\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.167454\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.251431\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.118525\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.064954\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.201354\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.112832\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.207746\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.151152\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.063883\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.075542\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.099213\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.064957\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.047372\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9707/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.072155\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.105351\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.033231\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.060370\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.111660\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.132917\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.043917\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.020911\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.030134\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.118504\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.028471\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.101591\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.075132\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.029601\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.166057\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.041440\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.068479\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.087290\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.033518\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.121771\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.178149\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.079611\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.039042\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.067281\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.036308\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.073480\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.057472\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.149584\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.016119\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.101386\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.184693\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.076443\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.055055\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.085466\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.103235\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.081402\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.097998\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.041874\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.079018\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.115462\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.023441\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.069105\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.061590\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.049610\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.079332\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.034953\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.125603\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.066814\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.060211\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.037059\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.035882\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.187106\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.055115\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.033409\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.195092\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.016170\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.224050\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.120719\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.056332\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.089114\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.190538\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.050796\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.024865\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.033424\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.061169\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.009128\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.088740\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.008401\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.017060\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.047080\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.085798\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.065871\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.030709\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.112923\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.087028\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.081411\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.083659\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.135401\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.093068\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.061719\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.100201\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.046664\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.125661\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.053343\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.075008\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.025524\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.042399\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.086131\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.048038\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.104809\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.039386\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.079133\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.178574\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.236811\n",
      "\n",
      "Test set: Average loss: 0.0711, Accuracy: 9779/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.043203\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.092515\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.081622\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.059474\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.149167\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.089270\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.016483\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.082490\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.026339\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.068515\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.022498\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.066867\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.030630\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.130916\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.069618\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.063571\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.045636\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.042675\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.110221\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.024695\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.018119\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.060834\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.052497\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.208511\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.063612\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.125993\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.087402\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.093278\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.071817\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.089041\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.029409\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.173989\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.034292\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.034481\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.054186\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.092428\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.056456\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.054807\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.027580\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.086341\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.018514\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.034095\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.080570\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.045136\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.080464\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.057863\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.064358\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.051244\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.017352\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.129134\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.030254\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.060472\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.011785\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.076003\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.042447\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.151303\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.055450\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.015287\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.107079\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.078401\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.041567\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.041418\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.058498\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.055844\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.136315\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.124109\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.194817\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.040863\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.226410\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.031954\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.128424\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.029676\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.062797\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.051813\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.025894\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.078441\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.234764\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.113535\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.066550\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.370890\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.070546\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.046498\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.167513\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.065704\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.093221\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.044461\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.028491\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.017374\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.024449\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.042214\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.094930\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.021994\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.087908\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.024526\n",
      "\n",
      "Test set: Average loss: 0.0639, Accuracy: 9789/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.040000\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.066195\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.098730\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.048507\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.068847\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.044480\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.132906\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.086539\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.099203\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.017589\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.134299\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.091914\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.074498\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.044972\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.264174\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.054054\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.048242\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.049260\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.078988\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.017076\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.019132\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.037874\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.029610\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.140147\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.033694\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.047073\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.063637\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.113439\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.062630\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.074932\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.036604\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.072447\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.047508\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.102421\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.051497\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.068158\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.061194\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.083473\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.030645\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.065479\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.047610\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.010869\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.084731\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.108994\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.079977\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.008496\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.055542\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.044294\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.063192\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.210488\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.042844\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.099760\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.069010\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.053456\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.020155\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.140261\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.022585\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.162339\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.050050\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.041625\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.099587\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.015581\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.074202\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.051868\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.027701\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.030844\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.045761\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.073721\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.047115\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.006762\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.145443\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.030145\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.052129\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.068803\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.059805\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.016771\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.095611\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.030376\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.077618\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.197935\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.277233\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.025948\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.059071\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.017358\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.020767\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.107295\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.076121\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.088041\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.040386\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.056009\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.088696\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.010071\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.090262\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.019514\n",
      "\n",
      "Test set: Average loss: 0.0609, Accuracy: 9818/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.028575\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.026072\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.053820\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.094004\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.058639\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.023699\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.010640\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.022844\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.053811\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.047533\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.023669\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.014490\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.147793\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.030916\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.041745\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.009600\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.094729\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.040353\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.017814\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.109979\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.134416\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.183224\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.174888\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.031328\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.043523\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.016490\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.103775\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.021387\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.009565\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.053518\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.170423\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.075955\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.038927\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.059367\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.078281\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.035243\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.079588\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.013791\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.084725\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.019888\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.071246\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.089697\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.034663\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.054511\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.087314\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.011897\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.085414\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.129305\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.022772\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.021406\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.164370\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.205581\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.024883\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.047946\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.046567\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.037408\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.010046\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.042129\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.170710\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.016590\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.051282\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.051970\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.047361\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.041623\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.008989\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.087155\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.030867\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.105476\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.018423\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.040557\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.020506\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.045383\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.036725\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.021756\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.111125\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.009254\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.060756\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.073978\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.035315\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.076708\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.124979\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.035435\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.077118\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.016680\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.050264\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.070629\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.116626\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.044900\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.058033\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.008724\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.077735\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.026087\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.025168\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.085031\n",
      "\n",
      "Test set: Average loss: 0.0498, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.074175\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.031538\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.032728\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.022967\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.086162\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.049916\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.088597\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.091238\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.019106\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.039973\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.018948\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.026369\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.007886\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.028737\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.038581\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.032766\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.095622\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.061773\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.252753\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.022412\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.069627\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.068933\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.064712\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.011296\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.029634\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.018600\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.121523\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.042521\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.019141\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.085693\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.018126\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.083108\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.036144\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.041332\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.028302\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.076192\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.013889\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.107234\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.043087\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.073668\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.015622\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.152449\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.156672\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.177518\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.227934\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.013938\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.034243\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.050969\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.059695\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.098901\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.081794\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.064752\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.034079\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.048142\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.015337\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.033042\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.020825\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.040559\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.033800\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.022073\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.060011\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.179067\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.005710\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.018812\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.035974\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.044981\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.071302\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.045592\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.118096\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.017468\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.029469\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.112496\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.011274\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.008845\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.087613\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.052197\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.050044\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.018767\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.056514\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.010712\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.034224\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.148979\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.016203\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.030370\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.022786\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.016376\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.134413\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.028967\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.045013\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.030148\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.095352\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.046365\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.061740\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.031475\n",
      "\n",
      "Test set: Average loss: 0.0520, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.162977\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.089938\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.016581\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.110446\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.037159\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.036744\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.057545\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.030302\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.005997\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.041714\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.115688\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.067667\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.067941\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.042097\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.077734\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.129516\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.021490\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.010408\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.016583\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.010561\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.037679\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.044084\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.109528\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.080565\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.015614\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.024609\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.015848\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.132334\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.063042\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.034764\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.038402\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.042590\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.030434\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.004803\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.227637\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.134902\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.229710\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.031422\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.026766\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.050833\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.058510\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.026028\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.025016\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.008599\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.011031\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.212886\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.068340\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.047440\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.024208\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.024375\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.048063\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.041984\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.012891\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.009989\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.106231\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.037272\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.067290\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.020707\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.101755\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.074797\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.031121\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.105659\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.013253\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.101376\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.053622\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.016487\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.163499\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.077892\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.044218\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.056239\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.036522\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.046813\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.031261\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.015060\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.026583\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.039175\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.051187\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.030197\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.118487\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.131583\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.101800\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.005277\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.030849\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.027856\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.017982\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.031016\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.048425\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.043941\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.052698\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.030653\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.053843\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.012131\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.557632\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.062736\n",
      "\n",
      "Test set: Average loss: 0.0507, Accuracy: 9833/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1398,  0.0492, -1.2330,  ...,  0.1329,  1.5521,  0.6990],\n",
      "         [-0.6062,  0.3039, -0.6295,  ..., -0.5077, -2.0444,  0.8801],\n",
      "         [ 1.3606, -1.5083, -1.8740,  ...,  0.4258,  0.8267,  1.2640],\n",
      "         ...,\n",
      "         [-0.2470, -0.5960,  0.3125,  ..., -1.6148, -0.9761,  0.3852],\n",
      "         [-0.0463,  0.8234, -0.1351,  ..., -1.2832,  0.5984, -0.1426],\n",
      "         [-0.3475, -1.3489, -0.0714,  ...,  0.3206,  0.7208, -2.2372]],\n",
      "\n",
      "        [[-0.0241,  0.2468, -0.7368,  ..., -0.1251, -1.0383, -1.5753],\n",
      "         [-0.7845, -0.4677, -1.2237,  ..., -0.4035, -0.2353,  0.7655],\n",
      "         [ 1.3126,  0.7105, -1.4776,  ...,  0.3350, -1.1821,  0.1837],\n",
      "         ...,\n",
      "         [-0.2918,  1.0135, -1.1932,  ...,  0.5947,  0.1615,  1.2094],\n",
      "         [ 0.7874, -0.3772,  1.1154,  ..., -0.0033,  0.3556,  1.6145],\n",
      "         [ 1.0081,  0.0604,  0.3051,  ..., -0.8557, -1.5964,  0.3333]],\n",
      "\n",
      "        [[-0.5826, -2.0345,  0.1530,  ...,  0.3437,  1.0712,  0.1986],\n",
      "         [ 0.3624,  0.4546, -1.0769,  ..., -0.2239, -0.9743, -1.4676],\n",
      "         [ 0.6528, -0.0175,  1.2237,  ...,  0.1602,  0.5792, -0.8377],\n",
      "         ...,\n",
      "         [-0.8573,  0.2597,  1.5593,  ..., -1.4571, -0.1645,  0.0824],\n",
      "         [ 0.0693,  0.3720, -0.2931,  ..., -1.1038, -1.2651, -1.0043],\n",
      "         [-1.0767,  1.2419,  1.1643,  ...,  0.0583, -1.0787, -1.4332]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.6829,  1.3958,  0.4787,  ...,  0.1286,  0.9158,  2.3152],\n",
      "         [-2.1205,  0.0440,  1.4969,  ...,  0.2160,  0.4232,  1.7432],\n",
      "         [-0.4233,  0.4447, -1.5609,  ...,  0.0787, -1.1837,  0.6685],\n",
      "         ...,\n",
      "         [-1.5312,  0.1098,  1.4442,  ..., -0.5337,  0.6812,  2.2453],\n",
      "         [-1.0958,  0.4895,  0.5604,  ...,  1.3575,  2.8235, -0.4734],\n",
      "         [-1.2260,  0.6259, -0.3238,  ...,  0.5296,  1.0186, -0.7384]],\n",
      "\n",
      "        [[ 1.0390,  0.9572, -0.5371,  ..., -0.8584, -0.0611, -0.6992],\n",
      "         [ 0.0198, -0.1354, -0.2736,  ...,  1.4572,  1.5313, -0.6279],\n",
      "         [ 0.1691, -0.3814, -1.5537,  ..., -0.0036, -0.0880,  1.2261],\n",
      "         ...,\n",
      "         [-1.1228, -0.2267,  1.3197,  ..., -0.0972,  0.0062,  0.9189],\n",
      "         [ 1.0118,  0.1340,  1.1800,  ..., -0.7410,  1.8237,  0.4864],\n",
      "         [-0.6780,  0.7253, -1.1000,  ..., -1.2987,  0.9924,  0.9798]],\n",
      "\n",
      "        [[ 0.2794,  0.7319,  1.4993,  ..., -0.3305, -1.3110,  1.0515],\n",
      "         [ 1.4291, -0.2898,  1.3989,  ...,  0.2187, -0.4592,  1.2746],\n",
      "         [-1.1814,  1.9518, -0.4427,  ..., -1.4206, -0.5868, -0.7679],\n",
      "         ...,\n",
      "         [-0.7630,  1.4597,  1.0819,  ..., -0.5566,  1.2224, -0.5614],\n",
      "         [ 0.5389,  1.9852, -0.6386,  ...,  0.7870, -0.7749,  0.8637],\n",
      "         [ 0.8454,  0.9023,  0.7065,  ..., -0.3457, -0.2982,  0.8137]]])\n"
     ]
    }
   ],
   "source": [
    " m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "print(input)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11_1_AdvancedCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309103\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.303921\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.298157\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.295048\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.298723\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.290981\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.295114\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.288141\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.307955\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.284344\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.274259\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.296044\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.266799\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.280445\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.274244\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.248479\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.240969\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.200672\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.158587\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.103314\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.880581\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.788703\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.466423\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.052457\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.033167\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.802999\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.616799\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.560305\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.898685\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.621075\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.484592\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.665458\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.410324\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.487895\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.310180\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.570926\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.367352\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.333488\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.321755\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.354969\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.352319\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.401809\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.375098\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.372209\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.387998\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.565645\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.349693\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.118272\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.262437\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.562547\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.184727\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.215332\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.232762\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.532955\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.401797\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.397969\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.369798\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.292457\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.363541\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.560054\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.418266\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.166849\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.303669\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.158693\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.340464\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.310682\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.198128\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.182248\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.242002\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.262081\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.118019\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.231971\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.218193\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.113245\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.320646\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.264185\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.122204\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.154243\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.327365\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.364962\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.156356\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.326029\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.190879\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.128882\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.129360\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.235922\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.180356\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.201920\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.360683\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.108777\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.326190\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.291672\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.266626\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.183204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1791, Accuracy: 9442/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.107685\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.146887\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.201372\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.236876\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.315779\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.154457\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.114969\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.232398\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.390035\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.104995\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.305182\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.110809\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.285429\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.301156\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.191527\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.093419\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.315737\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.133464\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.093832\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.051041\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.050675\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.070229\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.090459\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.252624\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.533676\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.363733\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.141184\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.299654\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.263844\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.206469\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.111537\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.201517\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.124972\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.088567\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.241385\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.319209\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.166150\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.061536\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.102866\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.100436\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.094563\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.123032\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.059652\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.034274\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.126765\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.095495\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.073977\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.123213\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.113385\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.070830\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.037615\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.215353\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.152061\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.049829\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.109980\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.149036\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.238380\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.113987\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.153556\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.069427\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.020734\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.256344\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.074669\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.087014\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.161930\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.088623\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.087237\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.145677\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.365668\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.096487\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.440514\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.135437\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.054872\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.084216\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.053030\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.064724\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.054839\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.042636\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.063097\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.113390\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.149605\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.082656\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.066617\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.096222\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.139182\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.121492\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.128611\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.181434\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.047717\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.132005\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.098222\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.075258\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.067685\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.139649\n",
      "\n",
      "Test set: Average loss: 0.1075, Accuracy: 9654/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.105593\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.042059\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.112153\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.026994\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.031168\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.106547\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.052332\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.140629\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.108207\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.085372\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.186258\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.055354\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.081215\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.093173\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.099964\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.212019\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.033927\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.085273\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.103714\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.022494\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.031690\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.128624\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.014698\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.021380\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.162684\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.078104\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.043681\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.176803\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.068153\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.084681\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.124839\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.099531\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.087699\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.041243\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.166766\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.107737\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.099335\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.095307\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.021712\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.219440\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.093322\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.177450\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.090967\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.186254\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.074859\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.049502\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.316703\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.203045\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.032672\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.035077\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.035349\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.105928\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.066019\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.062135\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.040144\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.055033\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.048719\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.232149\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.208664\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.090200\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.097030\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.088773\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.032854\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.077480\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.043493\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.072412\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.149996\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.146176\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.075335\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.053528\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.167740\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.138471\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.071976\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.129596\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.076122\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.035453\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.090721\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.068248\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.034631\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.071887\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.048491\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.142276\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.095812\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.032487\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.031916\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.043045\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.046270\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.062197\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.222698\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.307899\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.053114\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.211143\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.080597\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.092319\n",
      "\n",
      "Test set: Average loss: 0.0840, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.316981\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.033074\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.046941\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.062764\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.160562\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.022002\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.095760\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.117403\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.140265\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.128733\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.140804\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.083328\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.210136\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.051078\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.066783\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.166699\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.051158\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.094963\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.018253\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.185643\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.080323\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.065789\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.062606\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.126091\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.058661\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.104208\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.029932\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.123123\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.038543\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.037285\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.150709\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.042491\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.069969\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.058114\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.140963\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.056723\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.037955\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.037260\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.043222\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.181696\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.185156\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.060268\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.050360\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.021977\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.032375\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.105708\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.072859\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.027891\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.224212\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.206967\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.013431\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.068348\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.035670\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.121269\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.098834\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.043147\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.051230\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.051392\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.057078\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.028538\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.057234\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.028296\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.247164\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.223472\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.165105\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.119417\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.049489\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.066775\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.022870\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.206747\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.082682\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.048238\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.148938\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.150265\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.163059\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.007953\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.053887\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.205849\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.014593\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.045527\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.034298\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.025695\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.194958\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.100834\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.066591\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.087535\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.049250\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.054708\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.034754\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.015472\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.010036\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.030637\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.129275\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.024131\n",
      "\n",
      "Test set: Average loss: 0.0637, Accuracy: 9783/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.171756\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.020192\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.045061\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.115546\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.014294\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.086132\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.165028\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.064036\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.034494\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.061391\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.032290\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.008483\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.036104\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.202100\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.126081\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.098392\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.045530\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.087472\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.097161\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.080381\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.105579\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.016252\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.021811\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.013063\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.241506\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.025183\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.071177\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.154846\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.018604\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.040253\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.079314\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.078047\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.068151\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.205345\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.040271\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.030614\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.052817\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.051096\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.097928\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.081836\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.031426\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.029082\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.088187\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.117727\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.070198\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.032357\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.095047\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.061564\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.161570\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.033480\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.086333\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.069975\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.018636\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.037179\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.091351\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.045063\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.080133\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.140782\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.021898\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.037761\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.026996\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.071045\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.028965\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.074648\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.053922\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.182046\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.065211\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.092363\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.054644\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.044917\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.037076\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.090695\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.044469\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.272403\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.007641\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.074932\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.279194\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.023867\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.108863\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.011837\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.026642\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.121952\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.134097\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.111522\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.060574\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.090735\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.029975\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.041257\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.054131\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.032238\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.346848\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.065970\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.061418\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.103081\n",
      "\n",
      "Test set: Average loss: 0.0679, Accuracy: 9778/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.093364\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.052780\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.041020\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.120138\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.037868\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.050268\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.091744\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.166322\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.037298\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.054648\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.031596\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.055117\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.035007\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.077138\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.021726\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.012862\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.052317\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.033358\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.016291\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.050040\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.034143\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.122846\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.105906\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.105544\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.115688\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.032618\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.008905\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.155943\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.031134\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.037597\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.024487\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.210199\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.022440\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.049728\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.144330\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.034658\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.009334\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.057316\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.059089\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.028453\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.049035\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.122910\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.054736\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.018478\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.027505\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.062578\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.183172\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.052402\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.202560\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.068102\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.026587\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.063537\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.040246\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.110845\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.117195\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.039963\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.024701\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.066229\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.130913\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.035100\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.025343\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.102857\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.043470\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.053695\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.073784\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.105057\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.026289\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.066967\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.013735\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.141327\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.010118\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.214199\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.099480\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.085068\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.145418\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.184506\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.111326\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.048585\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.059364\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.075764\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.117647\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.093271\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.090902\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.026199\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.016250\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.083905\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.018006\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.029380\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.053643\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.067314\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.109525\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.111663\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.038022\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.091111\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.035500\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.018056\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.100120\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.124414\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.037299\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.120493\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.088224\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.040038\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.035363\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.016903\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.067538\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.436000\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.195867\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.031714\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.038021\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.064367\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.137200\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.053637\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.085489\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.067578\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.043390\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.068335\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.024014\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.029699\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.087923\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.137320\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.010159\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.021074\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.029217\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.068958\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.047047\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.005866\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.048978\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.142274\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.030451\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.063192\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.016517\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.007844\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.030930\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.072852\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.030311\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.015502\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.064202\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.019763\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.069144\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.010468\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.025450\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.073575\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.054998\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.050258\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.060905\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.021940\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.014820\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.077573\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.043755\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.020811\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.029689\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.009614\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.032267\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.007041\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.019700\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.067751\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.072931\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.017421\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.040500\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.104025\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.033575\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.145384\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.074568\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.059373\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.035860\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.109211\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.117727\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.048779\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.053831\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.012953\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.034092\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.160344\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.023738\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.017521\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.150876\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.073896\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.092627\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.019439\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.010126\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.042582\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.015923\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.045486\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.084920\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.120983\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.050299\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.097089\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.053409\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.001513\n",
      "\n",
      "Test set: Average loss: 0.0624, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.041654\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.006948\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.018130\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.018481\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.032546\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.016902\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.125853\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.098112\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.037358\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.010883\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.014826\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.032016\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.239763\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.058990\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.032673\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.052074\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.072500\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.012802\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.030012\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.076239\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.037696\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.014239\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.010125\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.124368\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.010736\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.037989\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.033885\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.079775\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.003815\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.146139\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.002363\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.007689\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.011587\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.024190\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.229169\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.044243\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.047566\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.013382\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.174057\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.082376\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.030434\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.035644\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.041189\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.035615\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.017885\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.041141\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.115921\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.077905\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.085811\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.010313\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.050027\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.014982\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.005719\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.117249\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.085867\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.012174\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.021205\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.133861\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.026835\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.075186\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.088179\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.004353\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.028863\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.004882\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.103969\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.052394\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.034275\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.016879\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.026541\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.119518\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.035075\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.031374\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.102062\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.097553\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.069305\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.165861\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.013351\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.048445\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.194309\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.104627\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.083234\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.089445\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.121248\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.013070\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.051220\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.021383\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.253138\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.100016\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.031025\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.010467\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.047770\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.013537\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.080904\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.016686\n",
      "\n",
      "Test set: Average loss: 0.0496, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.043105\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.023765\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.061881\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.113678\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.091040\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.019966\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.004022\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.035657\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.013965\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.032885\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.029578\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.066571\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.122539\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.016987\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.005275\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.034651\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.030599\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.015016\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.021671\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.052432\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.070078\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.025009\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.033176\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.014700\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.013878\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.034729\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.037399\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.030901\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.078217\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.019823\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.041231\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.131967\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.049133\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.076880\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.020735\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.015576\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.123791\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.070603\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.056005\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.033138\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.083865\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.093104\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.135276\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.084129\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.008364\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.035551\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.022657\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.092848\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.029872\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.038727\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.043824\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.027655\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.006307\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.047323\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.020602\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.057242\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.013163\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.033788\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.040627\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.086464\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.118854\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.088405\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.131252\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.020942\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.096496\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.009782\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.040693\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.044424\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.097399\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.010500\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.102138\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.019780\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.073547\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.022478\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.070713\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.006521\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.011170\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.057592\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.050033\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.056373\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.096399\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.007232\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.030658\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.006689\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.058508\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.009622\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.124132\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.080072\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.018001\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.172995\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.077240\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.051860\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.038775\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.009425\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9856/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12_1_RNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0.]]])\n",
      "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
      "tensor([[[0., 1., 0., 0.]]])\n",
      "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
      "tensor([[[0., 0., 1., 0.]]])\n",
      "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
      "tensor([[[0., 0., 1., 0.]]])\n",
      "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
      "tensor([[[0., 0., 0., 1.]]])\n",
      "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "\n",
    "# (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "hidden = Variable(torch.randn(1, 1, 2))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "#print(inputs)\n",
    "for one in inputs:\n",
    "    one = one.view(1, 1, -1)\n",
    "    print(one)\n",
    "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
    "    out, hidden = cell(one, hidden)\n",
    "    print(\"one input size\", one.size(), \"out size\", out.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]])\n",
      "sequence input size torch.Size([1, 5, 4]) out size torch.Size([1, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# We can do the whole at once\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "#print(inputs)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input size torch.Size([3, 5, 4]) out size torch.Size([3, 5, 2])\n",
      "batch input size torch.Size([5, 3, 4]) out size torch.Size([5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "hidden = Variable(torch.randn(1, 3, 2))\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "# 3 batches 'hello', 'eolll', 'lleel'\n",
    "# rank = (3, 5, 4)\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "# B x S x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size())\n",
    "\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2)\n",
    "cell = nn.RNN(input_size=4, hidden_size=2)\n",
    "\n",
    "# The given dimensions dim0 and dim1 are swapped.\n",
    "inputs = inputs.transpose(dim0=0, dim1=1)\n",
    "# Propagate input through RNN\n",
    "# Input: (seq_len, batch_size, input_size) when batch_first=False (default)\n",
    "# S x B x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12_2_HelloRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 12 RNN\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "#            0    1    2    3    4\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [0, 1, 0, 2, 3, 3]   # hihell\n",
    "one_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n",
    "                  [0, 1, 0, 0, 0],  # 1\n",
    "                  [0, 0, 1, 0, 0],  # 2\n",
    "                  [0, 0, 0, 1, 0],  # 3\n",
    "                  [0, 0, 0, 0, 1]]  # 4\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    " #As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the RNN. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 1  # One by one\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, hidden, x):\n",
    "        # Reshape input (batch first)\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        return hidden, out.view(-1, num_classes)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      ")\n",
      "predicted string: l"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-207c4bf4c929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", epoch: %d, loss: %1.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate RNN model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    sys.stdout.write(\"predicted string: \")\n",
    "    for input, label in zip(inputs, labels):\n",
    "        # print(input.size(), label.size())\n",
    "        hidden, output = model(hidden, input)\n",
    "        val, idx = output.max(1)\n",
    "        sys.stdout.write(idx2char[idx.data[0]])\n",
    "        loss += criterion(output, label)\n",
    "\n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12_3_HelloRNN_Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.]]])\n",
      "torch.Size([1, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "# Lab 12 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "print(inputs)\n",
    "print(inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        # Reshape input\n",
    "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x, h_0)\n",
    "        return out.view(-1, num_classes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      ")\n",
      "epoch: 1, loss: 1.693\n",
      "Predicted string:  llllll\n",
      "epoch: 2, loss: 1.523\n",
      "Predicted string:  llllll\n",
      "epoch: 3, loss: 1.393\n",
      "Predicted string:  llllll\n",
      "epoch: 4, loss: 1.263\n",
      "Predicted string:  llllll\n",
      "epoch: 5, loss: 1.146\n",
      "Predicted string:  llllll\n",
      "epoch: 6, loss: 1.055\n",
      "Predicted string:  lhelll\n",
      "epoch: 7, loss: 1.002\n",
      "Predicted string:  ihelll\n",
      "epoch: 8, loss: 0.965\n",
      "Predicted string:  ihelll\n",
      "epoch: 9, loss: 0.913\n",
      "Predicted string:  ihelll\n",
      "epoch: 10, loss: 0.879\n",
      "Predicted string:  ihelll\n",
      "epoch: 11, loss: 0.840\n",
      "Predicted string:  ihelll\n",
      "epoch: 12, loss: 0.805\n",
      "Predicted string:  ihello\n",
      "epoch: 13, loss: 0.779\n",
      "Predicted string:  ihello\n",
      "epoch: 14, loss: 0.758\n",
      "Predicted string:  ihello\n",
      "epoch: 15, loss: 0.738\n",
      "Predicted string:  ihello\n",
      "epoch: 16, loss: 0.717\n",
      "Predicted string:  ihello\n",
      "epoch: 17, loss: 0.694\n",
      "Predicted string:  ihello\n",
      "epoch: 18, loss: 0.667\n",
      "Predicted string:  ihelll\n",
      "epoch: 19, loss: 0.643\n",
      "Predicted string:  ihelll\n",
      "epoch: 20, loss: 0.647\n",
      "Predicted string:  ihelll\n",
      "epoch: 21, loss: 0.628\n",
      "Predicted string:  ihelll\n",
      "epoch: 22, loss: 0.607\n",
      "Predicted string:  ihelll\n",
      "epoch: 23, loss: 0.600\n",
      "Predicted string:  ihelll\n",
      "epoch: 24, loss: 0.596\n",
      "Predicted string:  ihello\n",
      "epoch: 25, loss: 0.591\n",
      "Predicted string:  ihello\n",
      "epoch: 26, loss: 0.583\n",
      "Predicted string:  ihello\n",
      "epoch: 27, loss: 0.573\n",
      "Predicted string:  ihello\n",
      "epoch: 28, loss: 0.562\n",
      "Predicted string:  ihello\n",
      "epoch: 29, loss: 0.550\n",
      "Predicted string:  ihello\n",
      "epoch: 30, loss: 0.540\n",
      "Predicted string:  ihello\n",
      "epoch: 31, loss: 0.527\n",
      "Predicted string:  ihello\n",
      "epoch: 32, loss: 0.524\n",
      "Predicted string:  ihello\n",
      "epoch: 33, loss: 0.530\n",
      "Predicted string:  ihello\n",
      "epoch: 34, loss: 0.519\n",
      "Predicted string:  ihello\n",
      "epoch: 35, loss: 0.507\n",
      "Predicted string:  ihello\n",
      "epoch: 36, loss: 0.503\n",
      "Predicted string:  ihello\n",
      "epoch: 37, loss: 0.503\n",
      "Predicted string:  ihello\n",
      "epoch: 38, loss: 0.500\n",
      "Predicted string:  ihello\n",
      "epoch: 39, loss: 0.496\n",
      "Predicted string:  ihello\n",
      "epoch: 40, loss: 0.494\n",
      "Predicted string:  ihello\n",
      "epoch: 41, loss: 0.493\n",
      "Predicted string:  ihello\n",
      "epoch: 42, loss: 0.492\n",
      "Predicted string:  ihello\n",
      "epoch: 43, loss: 0.488\n",
      "Predicted string:  ihello\n",
      "epoch: 44, loss: 0.484\n",
      "Predicted string:  ihello\n",
      "epoch: 45, loss: 0.481\n",
      "Predicted string:  ihello\n",
      "epoch: 46, loss: 0.481\n",
      "Predicted string:  ihello\n",
      "epoch: 47, loss: 0.480\n",
      "Predicted string:  ihello\n",
      "epoch: 48, loss: 0.477\n",
      "Predicted string:  ihello\n",
      "epoch: 49, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 50, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 51, loss: 0.475\n",
      "Predicted string:  ihello\n",
      "epoch: 52, loss: 0.473\n",
      "Predicted string:  ihello\n",
      "epoch: 53, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 54, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 55, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 56, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 57, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 58, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 59, loss: 0.468\n",
      "Predicted string:  ihello\n",
      "epoch: 60, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 61, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 62, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 63, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 64, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 65, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 66, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 67, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 68, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 69, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 70, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 71, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 72, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 73, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 74, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 75, loss: 0.462\n",
      "Predicted string:  ihello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 76, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 77, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 78, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 79, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 80, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 81, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 82, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 83, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 84, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 85, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 86, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 87, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 88, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 89, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 90, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 91, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 92, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 93, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 94, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 95, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 96, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 97, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 98, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 99, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 100, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RNN model\n",
    "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
    "print(rnn)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = rnn(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12_4_HelloRNNEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lab 12 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.LongTensor(x_data))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5\n",
    "embedding_size = 10  # embedding size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size,\n",
    "                          hidden_size=5, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(batch_size, sequence_length, -1)\n",
    "\n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, _ = self.rnn(emb, h_0)\n",
    "        return self.fc(out.view(-1, num_classes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(5, 10)\n",
      "  (rnn): RNN(10, 5, batch_first=True)\n",
      "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-504174ff2762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-3be2a9c7cf05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# (num_layers * num_directions, batch, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         h_0 = Variable(torch.zeros(\n\u001b[0;32m---> 14\u001b[0;31m             self.num_layers, x.size(0), self.hidden_size))\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'num_layers'"
     ]
    }
   ],
   "source": [
    "# Instantiate RNN model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = model(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
