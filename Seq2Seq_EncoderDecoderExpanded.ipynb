{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Seq2Seq to Transformers\n",
    "- based on https://github.com/bentrevett/pytorch-seq2seq\n",
    "\n",
    "### 1.  [Sequence to Sequence Learning with Neural Networks ](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "This introduced the basics of seq2seq networks using encoder-decoder models. The model itself will be based off an implementation of Sequence to Sequence Learning with Neural Networks, which uses multi-layer LSTMs.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*hG-VIciA7fmGYcSKD9YhZg.png\" width=\"50%\">\n",
    "- seq2seq with an input sequence of length 4\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*erEV0fHM233twhYPhw3gvg.png\" width=\"80%\">\n",
    "- seq2seq with an input sequence of length 64\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq4.png\" width=\"80%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "This covers a second model, which helps with the information compression problem faced by encoder-decoder models. This model will be based off an implementation of Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, which uses GRUs.\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq7.png\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "We learn about **attention** by implementing Neural Machine Translation by Jointly Learning to Align and Translate. This further allievates the information compression problem by allowing the decoder to \"look back\" at the input sentence by creating context vectors that are weighted sums of the encoder hidden states. The weights for this weighted sum are calculated via an attention mechanism, where the decoder learns to pay attention to the most relevant words in the input sentence.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xCQbNIXsHxaEU0MZXDlAHQ.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq10.png\" width=\"50%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Packed Padded Sequences, Masking, Inference and BLEU Open In Colab\n",
    "\n",
    "We will improve the previous model architecture by adding packed padded sequences and masking. These are two methods commonly used in NLP. Packed padded sequences allow us to only process the non-padded elements of our input sentence with our RNN. Masking is used to force the model to ignore certain elements we do not want it to look at, such as attention over padded elements. Together, these give us a small performance boost. We also cover a very basic way of using the model for inference, allowing us to get translations for any sentence we want to give to the model and how we can view the attention values over the source sequence for those translations. Finally, we show how to calculate the BLEU metric from our translations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) \n",
    "\n",
    "We finally move away from RNN based models and implement a fully convolutional model. One of the downsides of RNNs is that they are sequential. That is, before a word is processed by the RNN, all previous words must also be processed. Convolutional models can be fully parallelized, which allow them to be trained much quicker. We will be implementing the Convolutional Sequence to Sequence model, which uses multiple convolutional layers in both the encoder and decoder, with an attention mechanism between them.\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/convseq2seq0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Continuing with the non-RNN based models, we implement the Transformer model from Attention Is All You Need. This model is based soley on attention mechanisms and introduces Multi-Head Attention. The encoder and decoder are made of multiple layers, with each layer consisting of Multi-Head Attention and Positionwise Feedforward sublayers. This model is currently used in many state-of-the-art sequence-to-sequence and transfer learning tasks.\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/transformer1.png\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Basics\n",
    "\n",
    "Formally, in the machine translation task, we have an input sequence $x_1, x_2, \\dots, x_m$ and an output sequence $y_1, y_2, \\dots, y_n$ (note that their lengths can be different). Translation can be thought of as finding the target sequence that is the most probable given the input; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\\ast}=\\arg\\max\\limits_{y}p(y|x).$\n",
    "\n",
    "If you are bilingual and can translate between languages easily, you have an intuitive feeling of $p(y|x)$ and can say something like \"...well, this translation is kind of more natural for this sentence\". But in machine translation, we learn a function $p(y|x, \\theta)$ with some parameters $\\theta$, and then find its argmax for a given input: $y'=\\arg\\max\\limits_{y}p(y|x, \\theta).$\n",
    "\n",
    "To define a machine translation system, we need to answer three questions:\n",
    "\n",
    "   - modeling - how does the model for $p(y|x, \\theta)$ look like?\n",
    "   - learning - how to find the parameters $\\theta$?\n",
    "   - inference - how to find the best $y$?\n",
    "\n",
    "In this section, we will answer the second and third questions in full, but consider only the simplest model. The more \"real\" models will be considered later in sections Attention and Transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Framework\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec-min.png\" width=\"50%\" align=\"right\">\n",
    "\n",
    "Encoder-decoder is the standard modeling paradigm for sequence-to-sequence tasks. This framework consists of two components:\n",
    "\n",
    "- encoder - reads source sequence and produces its representation;\n",
    "- decoder - uses source representation from the encoder to generate the target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Language Models\n",
    "\n",
    "In the Language Modeling lecture, we learned to estimate the probability $p(y)$ of sequences of tokens $y=(y_1, y_2, \\dots, y_n)$. While language models estimate the unconditional probability $p(y)$ of a sequence $y$, sequence-to-sequence models need to estimate the conditional probability p(y|x) of a sequence $y$ given a source $x$. That's why sequence-to-sequence tasks can be modeled as Conditional Language Models (CLM) - they operate similarly to LMs, but additionally receive source information $x$.\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/lm_clm-min.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_prob_idea.gif\" width=\"50%\">\n",
    "\n",
    "Since the only difference from LMs is the presence of source , the modeling and training is very similar to language models. In particular, the high-level pipeline is as follows:\n",
    "\n",
    "- feed source and previously generated target words into a network;\n",
    "- get vector representation of context (both source and previous target) from the networks decoder;\n",
    "- from this vector representation, predict a probability distribution for the next token.\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_linear_out-min.png\" width=\"80%\">\n",
    "\n",
    "Similarly to neural classifiers and language models, we can think about the classification part (i.e., how to get token probabilities from a vector representation of a text) in a very simple way. Vector representation of a text has some dimensionality , but in the end, we need a vector of size  (probabilities for  tokens/classes). To get a -sized vector from a -sized, we can use a linear layer. Once we have a -sized vector, all is left is to apply the softmax operation to convert the raw numbers into token probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simplest Model: Two RNNs for Encoder and Decoder\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_simple_rnn-min.png\" width=\"80%\">\n",
    "\n",
    "The simplest encoder-decoder model consists of two RNNs (LSTMs): one for the encoder and another for the decoder. Encoder RNN reads the source sentence, and the final state is used as the initial state of the decoder RNN. The hope is that the final encoder state \"encodes\" all information about the source, and the decoder can generate the target sentence based on this vector.\n",
    "\n",
    "This model can have different modifications: for example, the encoder and decoder can have several layers. Such a model with several layers was used, for example, in the paper [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf) - one of the first attempts to solve sequence-to-sequence tasks using neural networks.\n",
    "\n",
    "In the same paper, the authors looked at the last encoder state and visualized several examples - look below. Interestingly, representations of sentences with similar meaning but different structure are close!\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/rnn_simple_examples-min.png\" width=\"80%\">\n",
    "The examples are from the paper Sequence to Sequence Learning with Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: The Cross-Entropy Loss (Once Again)\n",
    "\n",
    "Similarly to neural LMs, neural seq2seq models are trained to predict probability distributions of the next token given previous context (source and previous target tokens). Intuitively, at each step we maximize the probability a model assigns to the correct token.\n",
    "\n",
    "Formally, let's assume we have a training instance with the source $x=(x_1, \\dots, x_m)$ and the target $y=(y_1, \\dots, y_n)$. Then at the timestep $t$, a model predicts a probability distribution $p^{(t)} = p(\\ast|y_1, \\dots, y_{t-1}, x_1, \\dots, x_m)$. The target at this step is $p^{\\ast}=\\mbox{one-hot}(y_t)$, i.e., we want a model to assign probability 1 to the correct token, $y_t$, and zero to the rest.\n",
    "\n",
    "The standard loss function is the cross-entropy loss. Cross-entropy loss for the target distribution $p^{\\ast}$ and the predicted distribution $p^{}$ is $$Loss(p^{\\ast}, p^{})= - p^{\\ast} \\log(p) = -\\sum\\limits_{i=1}^{|V|}p_i^{\\ast} \\log(p_i).$$ \n",
    "Since only one of $p_i^{\\ast}$ is non-zero (for the correct token $y_t$), we will get $$Loss(p^{\\ast}, p) = -\\log(p_{y_t})=-\\log(p(y_t| y_{\\mbox{<}t}, x)).$$\n",
    "At each step, we maximize the probability a model assigns to the correct token. Look at the illustration for a single timestep.\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/one_step_loss_intuition-min.png\" width=\"80%\">\n",
    "\n",
    "For the whole example, the loss will be $-\\sum\\limits_{t=1}^n\\log(p(y_t| y_{\\mbox{<}t}, x))$. Look at the illustration of the training process (the illustration is for the RNN model, but the model can be different). \n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/seq2seq_training_with_target.gif\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Greedy Decoding and Beam Search\n",
    "\n",
    "Now when we understand how a model can look like and how to train this model, let's think how to generate a translation using this model. We model the probability of a sentence as follows:\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/inference_formula-min.png\" width=\"80%\">\n",
    "\n",
    "Now the main question is: how to find the argmax?\n",
    "\n",
    "Note that **we can not find the exact solution.** The total number of hypotheses we need to check is $|V|^n$,\n",
    ", which is not feasible in practice. Therefore, we will find an approximate solution.\n",
    "\n",
    "\n",
    "• **Greedy Decoding:** At each step, pick the most probable token\n",
    "The straightforward decoding strategy is greedy - at each step, generate a token with the highest probability. This can be a good baseline, but this method is inherently flawed: the best token at the current step does not necessarily lead to the best sequence.\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/greedy_is_bad-min.png\" width=\"80%\">\n",
    "\n",
    "• **Beam Search:** Keep track of several most probably hypotheses\n",
    "Instead, let's keep several hypotheses. At each step, we will be continuing each of the current hypotheses and pick top-N of them. This is called beam search.\n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/beam_search.gif\" width=\"80%\">\n",
    "\n",
    "Usually, the beam size is 4-10. Increasing beam size is computationally inefficient and, what is more important, leads to worse quality.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
